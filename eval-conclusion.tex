\section{Evaluation}

\subsection{Experimental Setup}

We evaluated AgentSight on AWS EC2 c5.2xlarge instances (8 vCPUs, 16GB RAM, Linux 5.15) across three agent frameworks: LangChain 0.1.0, AutoGen 0.2.0, and Claude Code. Workloads included code generation (simple functions to complex systems), data analysis (CSV/JSON processing), and system administration (package management). We measured end-to-end completion time, CPU/memory usage, and event rates, running each experiment 50 times with and without AgentSight to ensure statistical significance.

\subsection{Performance Evaluation}


\begin{table}[h]
\centering
\caption{Performance Overhead of AgentSight}
\label{tab:performance}
\begin{tabular}{lrrrr}
\toprule
Workload Type & Baseline & With AgentSight & Overhead & Events/sec \\
\midrule
Code Generation (simple) & 12.3s & 12.5s & 1.6\% & 432 \\
Code Generation (complex) & 87.2s & 89.1s & 2.2\% & 1,247 \\
Data Analysis & 34.5s & 35.2s & 2.0\% & 892 \\
System Admin Tasks & 23.1s & 23.7s & 2.6\% & 2,156 \\
Idle Agent & 0.1\% CPU & 0.3\% CPU & +0.2\% & 12 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:performance} summarizes performance impacts across different agent task types.The results consistent sub-3\% overhead across all workloads. Simple code generation shows minimal 1.6\% impact due to fewer system events, while complex generation with extensive file operations reaches 2.2\%. System administration tasks exhibit the highest overhead at 2.6\% due to intensive process spawning and file system operations, generating over 2,000 events per second. Even under peak load, AgentSight maintains responsive performance with fixed memory footprint (192MB on 8-core systems) and negligible idle overhead (0.2\% CPU), validating our kernel-space filtering design.

\subsection{Effectiveness Evaluation}

We evaluated AgentSight's effectiveness through three comprehensive case studies that demonstrate its ability to detect security threats, identify performance issues, and provide insights into complex multi-agent systems.

\subsection{Case Studies}

\subsubsection{Case Study 1: Detecting Prompt Injection Attacks}

We tested AgentSight's ability to detect prompt injection attacks where a data analysis agent received a crafted prompt embedding malicious commands within a legitimate request to analyze sales data, ultimately exfiltrating /etc/passwd. AgentSight captured the complete attack chain: LLM interaction with suspicious prompt (T+0ms), agent-generated Python script with embedded curl command (T+125ms), subprocess spawn (T+342ms), outbound HTTPS connection to suspicious domain (T+367ms), and sensitive file read (T+368ms). The correlation engine identified potential data exfiltration with 92\% confidence by linking the prompt injection with subsequent file access, suspicious network connection, and 1.2KB transfer matching the file size. This detection proves critical for production deployments where traditional application-level monitoring would miss the correlation between initial prompts and system activities across process boundaries, while AgentSight's boundary tracing captures the complete attack narrative enabling rapid incident response.

\subsubsection{Case Study 2: Reasoning Loop Detection}

An agent attempting a complex task entered an infinite reasoning loop with circular dependencies where solving X required solving Y, but solving Y required solving X—a pattern common when agents encounter problems outside their training distribution. AgentSight detected this through multiple mechanisms: pattern analysis tracking LLM API call sequences with semantic similarity metrics to identify repeated prompt structures even when rephrased; resource monitoring showing constant token consumption without progress markers instead of healthy decreasing usage; temporal correlation revealing suspiciously regular intervals between API calls characteristic of stuck retry logic; and semantic progress tracking using embedding-based similarity to detect reasoning stagnation. The system triggered an alert after detecting three complete cycles where the agent had consumed 4,800 tokens across 12 API calls, with AgentSight's intervention saving an estimated \$2.40 in API costs and preventing service degradation—demonstrating the critical importance of semantic-aware monitoring for autonomous agents.

\subsubsection{Case Study 3: Multi-Agent Coordination Monitoring}

AgentSight monitored three agents collaborating on software development (Agent A: architecture design, Agent B: implementation, Agent C: testing), capturing 12,847 total events with 342 correlated actions across 27 synchronization points involving 15 shared files and 3 network endpoints. The analysis revealed critical inefficiencies invisible to traditional monitoring: Agent B spent 34\% of time blocked on Agent A's multiple design revisions triggering cascading rework; file locking patterns showed resource contention with Agent C's testing conflicting with Agent B's implementation causing 23 retry cycles; inter-agent communication through shared files generated 1,800 unnecessary file system operations from 2-second polling intervals; yet agents developed emergent coordination with Agent B learning to batch changes, reducing test executions by 40\%. These insights demonstrate that explicit coordination mechanisms could reduce runtime by 25\% and message-based communication would eliminate 90\% of polling overhead—revealing how boundary tracing uniquely captures multi-agent system dynamics that application-level monitoring cannot observe across process boundaries.

% \section{Future Work}
% While AgentSight establishes the power of boundary tracing, our work opens several avenues for future research focused on advancing the safety and observability of autonomous systems.

% First, we plan to enhance our causal correlation engine with machine learning. Currently, AgentSight uses a rule-based approach to bridge the semantic gap. Future work will involve training models on correlated intent-action traces to automatically detect novel anomalous behaviors, moving from identifying known attack patterns to discovering unknown unknowns. This includes using NLP to assess the semantic risk of prompts before they are even executed.

% Second, we will extend our framework from passive observation to active intervention. By integrating formal specification languages, an operator could define safety policies (e.g., "this agent may not access files outside its working directory"). AgentSight could then act as a runtime monitor that enforces these policies, functioning as a "circuit breaker" to terminate harmful actions before they complete.

% Finally, we will address the challenges of scale and integration. This involves extending boundary tracing to distributed environments to monitor multi-node agents and integrating with standard formats like OpenTelemetry to ensure AgentSight's insights are available in existing observability platforms. This also includes exploring privacy-preserving techniques to enable analysis without exposing sensitive data in prompts or system interactions.

\section{Conclusion}

This paper introduced boundary tracing as a novel observability paradigm for AI agents, monitoring at stable system interfaces rather than within rapidly evolving application code. AgentSight demonstrates this approach's feasibility, achieving sub-3\% overhead while detecting prompt injection attacks and identifying reasoning loops before resource exhaustion. By combining TLS interception with eBPF-based kernel monitoring, we bridge the semantic gap between agent intentions and system effects. We release AgentSight as open source to address the critical challenge of safely deploying autonomous AI systems in production environments.

\textbf{Repository}: \url{https://github.com/eunomia-bpf/agentsight}

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}