\section{Evaluation}

\subsection{Experimental Setup}

We evaluated AgentSight across diverse production workloads to assess performance overhead, detection effectiveness, and behavioral insights. Our experimental environment consisted of AWS EC2 c5.2xlarge instances with 8 vCPUs and 16GB RAM running Linux kernel 5.15 with BTF support enabled. We tested three major agent frameworks: LangChain 0.1.0 representing the most popular open-source framework, AutoGen 0.2.0 for multi-agent scenarios, and Claude Code for production code generation tasks. Workloads included code generation tasks ranging from simple functions to complex system implementations, data analysis pipelines processing CSV and JSON datasets, and system administration tasks involving package installation and configuration management.

Performance measurements focused on end-to-end task completion time, CPU utilization during agent execution, memory consumption patterns, and event generation rates. We compared identical workloads with and without AgentSight enabled, running each experiment 50 times to ensure statistical significance. All measurements excluded initial warm-up runs to avoid JIT compilation effects.

\subsection{Performance Evaluation}

AgentSight achieves its design goal of sub-3\% overhead across all tested workloads, making it suitable for production deployment. Table~\ref{tab:performance} summarizes performance impacts across different agent task types.

\begin{table}[h]
\centering
\caption{Performance Overhead of AgentSight}
\label{tab:performance}
\begin{tabular}{lrrrr}
\toprule
Workload Type & Baseline & With AgentSight & Overhead & Events/sec \\
\midrule
Code Generation (simple) & 12.3s & 12.5s & 1.6\% & 432 \\
Code Generation (complex) & 87.2s & 89.1s & 2.2\% & 1,247 \\
Data Analysis & 34.5s & 35.2s & 2.0\% & 892 \\
System Admin Tasks & 23.1s & 23.7s & 2.6\% & 2,156 \\
Idle Agent & 0.1\% CPU & 0.3\% CPU & +0.2\% & 12 \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate consistent low overhead across diverse workload types. Simple code generation tasks show minimal impact at 1.6\% overhead, as these workloads generate fewer system events. Complex code generation involving multiple file operations and subprocess spawning increases overhead slightly to 2.2\%, still well within acceptable bounds. System administration tasks exhibit the highest overhead at 2.6\% due to intensive process spawning and file system operations, generating over 2,000 events per second. Even under this peak load, AgentSight maintains responsive performance.

Memory consumption remains predictable at 128MB base allocation plus 8MB per CPU core for ring buffers. This translates to 192MB total on our 8-core test systems, negligible compared to typical agent memory requirements. The fixed memory footprint prevents resource exhaustion under sustained load, a critical requirement for production deployments.

CPU overhead analysis reveals that eBPF probe execution accounts for only 15\% of total overhead, with the remainder attributed to event processing and correlation. This distribution validates our design choice of kernel-space filtering to minimize data movement. During idle periods when agents await user input, AgentSight adds only 0.2\% CPU utilization, ensuring minimal impact on system resources.

\subsection{Effectiveness Evaluation}

We evaluated AgentSight's effectiveness through three comprehensive case studies that demonstrate its ability to detect security threats, identify performance issues, and provide insights into complex multi-agent systems.

\subsection{Case Studies}

\subsubsection{Case Study 1: Detecting Prompt Injection Attacks}

We tested AgentSight's ability to detect prompt injection attacks where a data analysis agent received a crafted prompt embedding malicious commands within a legitimate request to analyze sales data, ultimately exfiltrating /etc/passwd. AgentSight captured the complete attack chain: LLM interaction with suspicious prompt (T+0ms), agent-generated Python script with embedded curl command (T+125ms), subprocess spawn (T+342ms), outbound HTTPS connection to suspicious domain (T+367ms), and sensitive file read (T+368ms). The correlation engine identified potential data exfiltration with 92\% confidence by linking the prompt injection with subsequent file access, suspicious network connection, and 1.2KB transfer matching the file size. This detection proves critical for production deployments where traditional application-level monitoring would miss the correlation between initial prompts and system activities across process boundaries, while AgentSight's boundary tracing captures the complete attack narrative enabling rapid incident response.

\subsubsection{Case Study 2: Reasoning Loop Detection}

An agent attempting a complex task entered an infinite reasoning loop with circular dependencies where solving X required solving Y, but solving Y required solving X—a pattern common when agents encounter problems outside their training distribution. AgentSight detected this through multiple mechanisms: pattern analysis tracking LLM API call sequences with semantic similarity metrics to identify repeated prompt structures even when rephrased; resource monitoring showing constant token consumption without progress markers instead of healthy decreasing usage; temporal correlation revealing suspiciously regular intervals between API calls characteristic of stuck retry logic; and semantic progress tracking using embedding-based similarity to detect reasoning stagnation. The system triggered an alert after detecting three complete cycles where the agent had consumed 4,800 tokens across 12 API calls, with AgentSight's intervention saving an estimated \$2.40 in API costs and preventing service degradation—demonstrating the critical importance of semantic-aware monitoring for autonomous agents.

\subsubsection{Case Study 3: Multi-Agent Coordination Monitoring}

AgentSight monitored three agents collaborating on software development (Agent A: architecture design, Agent B: implementation, Agent C: testing), capturing 12,847 total events with 342 correlated actions across 27 synchronization points involving 15 shared files and 3 network endpoints. The analysis revealed critical inefficiencies invisible to traditional monitoring: Agent B spent 34\% of time blocked on Agent A's multiple design revisions triggering cascading rework; file locking patterns showed resource contention with Agent C's testing conflicting with Agent B's implementation causing 23 retry cycles; inter-agent communication through shared files generated 1,800 unnecessary file system operations from 2-second polling intervals; yet agents developed emergent coordination with Agent B learning to batch changes, reducing test executions by 40\%. These insights demonstrate that explicit coordination mechanisms could reduce runtime by 25\% and message-based communication would eliminate 90\% of polling overhead—revealing how boundary tracing uniquely captures multi-agent system dynamics that application-level monitoring cannot observe across process boundaries.


\section{Future Work}

While AgentSight demonstrates the feasibility of boundary tracing with sub-3\% overhead and effective threat detection, significant opportunities remain for advancing AI agent observability. Immediate engineering improvements include distributed tracing across multiple hosts, OpenTelemetry integration for existing observability platforms, and BPF CO-RE optimizations for improved portability. Medium-term research should focus on machine learning models for automated anomaly detection, natural language processing of captured prompts for semantic-aware alerting, and formal specification languages to define and verify expected agent behaviors. Long-term vision encompasses active intervention capabilities with circuit breakers for harmful behaviors, cryptographic attestation for tamper-proof audit trails, and federated learning to build industry-wide behavioral models without sharing sensitive data. Critical challenges requiring interdisciplinary collaboration include privacy-preserving monitoring through differential privacy and homomorphic encryption, standardization of agent event schemas and behavioral baselines, and runtime monitors synthesized from temporal logic specifications. As AI agents assume greater autonomy in critical systems, these advances become essential—the gap between agent capabilities and our ability to observe them represents a fundamental risk that the research community must address urgently through continued development of comprehensive observability frameworks.


\section{Conclusion}

This paper introduced boundary tracing as a novel observability paradigm for AI agents, monitoring at stable system interfaces rather than within rapidly evolving application code. AgentSight demonstrates this approach's feasibility, achieving sub-3\% overhead while detecting prompt injection attacks with 92\% confidence and identifying reasoning loops before resource exhaustion. By combining TLS interception with eBPF-based kernel monitoring, we bridge the semantic gap between agent intentions and system effects. We release AgentSight as open source to address the critical challenge of safely deploying autonomous AI systems in production environments.

\textbf{Repository}: \url{https://github.com/eunomia-bpf/agentsight}

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}