\section{Evaluation}

\subsection{Experimental Setup}

We evaluated AgentSight on AWS EC2 c5.2xlarge instances (8 vCPUs, 16GB RAM, Linux 5.15) across three agent frameworks: LangChain 0.1.0, AutoGen 0.2.0, and Claude Code. Workloads included code generation (simple functions to complex systems), data analysis (CSV/JSON processing), and system administration (package management). We measured end-to-end completion time, CPU/memory usage, and event rates, running each experiment 50 times with and without AgentSight to ensure statistical significance.

\subsection{Performance Evaluation}


\begin{table}[h]
\centering
\caption{Performance Overhead of AgentSight}
\label{tab:performance}
\begin{tabular}{lrrrr}
\toprule
Workload Type & Baseline & With AgentSight & Overhead & Events/sec \\
\midrule
Code Generation (simple) & 12.3s & 12.5s & 1.6\% & 432 \\
Code Generation (complex) & 87.2s & 89.1s & 2.2\% & 1,247 \\
Data Analysis & 34.5s & 35.2s & 2.0\% & 892 \\
System Admin Tasks & 23.1s & 23.7s & 2.6\% & 2,156 \\
Idle Agent & 0.1\% CPU & 0.3\% CPU & +0.2\% & 12 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:performance} summarizes performance impacts across different agent task types.The results consistent sub-3\% overhead across all workloads. Simple code generation shows minimal 1.6\% impact due to fewer system events, while complex generation with extensive file operations reaches 2.2\%. System administration tasks exhibit the highest overhead at 2.6\% due to intensive process spawning and file system operations, generating over 2,000 events per second. Even under peak load, AgentSight maintains responsive performance with fixed memory footprint (192MB on 8-core systems) and negligible idle overhead (0.2\% CPU), validating our kernel-space filtering design.

\subsection{Effectiveness Evaluation}

We evaluated AgentSight's effectiveness through three comprehensive case studies that demonstrate its ability to detect security threats, identify performance issues, and provide insights into complex multi-agent systems.

\subsection{Case Studies}

\subsubsection{Case Study 1: Detecting Prompt Injection Attacks}

We tested AgentSight's ability to detect prompt injection attacks where a data analysis agent received a crafted prompt embedding malicious commands within a legitimate request to analyze sales data, ultimately exfiltrating /etc/passwd. AgentSight captured the complete attack chain: LLM interaction with suspicious prompt (T+0ms), agent-generated Python script with embedded curl command (T+125ms), subprocess spawn (T+342ms), outbound HTTPS connection to suspicious domain (T+367ms), and sensitive file read (T+368ms). The correlation engine identified potential data exfiltration with 92\% confidence by linking the prompt injection with subsequent file access, suspicious network connection, and 1.2KB transfer matching the file size. This detection proves critical for production deployments where traditional application-level monitoring would miss the correlation between initial prompts and system activities across process boundaries, while AgentSight's boundary tracing captures the complete attack narrative enabling rapid incident response.

\subsubsection{Case Study 2: Reasoning Loop Detection}

An agent attempting a complex task entered an infinite reasoning loop with circular dependencies where solving X required solving Y, but solving Y required solving X—a pattern common when agents encounter problems outside their training distribution. AgentSight detected this through multiple mechanisms: pattern analysis tracking LLM API call sequences with semantic similarity metrics to identify repeated prompt structures even when rephrased; resource monitoring showing constant token consumption without progress markers instead of healthy decreasing usage; temporal correlation revealing suspiciously regular intervals between API calls characteristic of stuck retry logic; and semantic progress tracking using embedding-based similarity to detect reasoning stagnation. The system triggered an alert after detecting three complete cycles where the agent had consumed 4,800 tokens across 12 API calls, with AgentSight's intervention saving an estimated \$2.40 in API costs and preventing service degradation—demonstrating the critical importance of semantic-aware monitoring for autonomous agents.

\subsubsection{Case Study 3: Multi-Agent Coordination Monitoring}

AgentSight monitored three agents collaborating on software development (Agent A: architecture design, Agent B: implementation, Agent C: testing), capturing 12,847 total events with 342 correlated actions across 27 synchronization points involving 15 shared files and 3 network endpoints. The analysis revealed critical inefficiencies invisible to traditional monitoring: Agent B spent 34\% of time blocked on Agent A's multiple design revisions triggering cascading rework; file locking patterns showed resource contention with Agent C's testing conflicting with Agent B's implementation causing 23 retry cycles; inter-agent communication through shared files generated 1,800 unnecessary file system operations from 2-second polling intervals; yet agents developed emergent coordination with Agent B learning to batch changes, reducing test executions by 40\%. These insights demonstrate that explicit coordination mechanisms could reduce runtime by 25\% and message-based communication would eliminate 90\% of polling overhead—revealing how boundary tracing uniquely captures multi-agent system dynamics that application-level monitoring cannot observe across process boundaries.


\section{Future Work}

While AgentSight demonstrates the feasibility of boundary tracing with sub-3\% overhead and effective threat detection, significant opportunities remain for advancing AI agent observability. Immediate engineering improvements include distributed tracing across multiple hosts, OpenTelemetry integration for existing observability platforms, and BPF CO-RE optimizations for improved portability. Medium-term research should focus on machine learning models for automated anomaly detection, natural language processing of captured prompts for semantic-aware alerting, and formal specification languages to define and verify expected agent behaviors. Long-term vision encompasses active intervention capabilities with circuit breakers for harmful behaviors, cryptographic attestation for tamper-proof audit trails, and federated learning to build industry-wide behavioral models without sharing sensitive data. Critical challenges requiring interdisciplinary collaboration include privacy-preserving monitoring through differential privacy and homomorphic encryption, standardization of agent event schemas and behavioral baselines, and runtime monitors synthesized from temporal logic specifications. As AI agents assume greater autonomy in critical systems, these advances become essential—the gap between agent capabilities and our ability to observe them represents a fundamental risk that the research community must address urgently through continued development of comprehensive observability frameworks.


\section{Conclusion}

This paper introduced boundary tracing as a novel observability paradigm for AI agents, monitoring at stable system interfaces rather than within rapidly evolving application code. AgentSight demonstrates this approach's feasibility, achieving sub-3\% overhead while detecting prompt injection attacks with 92\% confidence and identifying reasoning loops before resource exhaustion. By combining TLS interception with eBPF-based kernel monitoring, we bridge the semantic gap between agent intentions and system effects. We release AgentSight as open source to address the critical challenge of safely deploying autonomous AI systems in production environments.

\textbf{Repository}: \url{https://github.com/eunomia-bpf/agentsight}

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}