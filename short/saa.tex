
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment
\usepackage{fontspec}   % For custom font symbols

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line
\newunicodechar{☁}{{\fontspec{Symbola}\symbol{"2601}}} % Cloud symbol

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
}


\sloppy
\begin{abstract}
AI agent systems are transforming software infrastructure through autonomous task orchestration, but their production deployment reveals a fundamental mismatch: traditional observability assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving internal logic. Current solutions—traditional APM, model-centric LLM monitoring, and framework-specific instrumentation—create fragmented SDK silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. We propose a two-plane architecture: (1) a Data Plane providing zero-instrumentation telemetry capture at stable system boundaries (kernel, network, TLS), decoupling observability from agent internals, and (2) a Cognitive Plane where autonomous observability agents provide semantic understanding and proactive management that human operators cannot achieve at scale. This approach addresses the unique demands of agent systems while enabling secure, cost-effective, and semantically-aware observability.
\end{abstract>


\maketitle



\section{Introduction: The Agent Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows: debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state—all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{1. Semantic Failures Replace Deterministic Errors.} In our code review example, the agent might hallucinate a security vulnerability that doesn't exist, enter an infinite loop requesting more context, or forget critical style guidelines mid-review. Unlike traditional crashes (segfaults, exceptions), these failures are \emph{semantic}—plausible but incorrect outputs that require understanding agent \emph{intent} to detect. More critically, prompt injection attacks~\cite{indirect-prompt-inject} can compromise agents to evade their own logging, hiding malicious behavior. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{2. Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer: token usage for LLM calls, vector database queries, API calls to GitHub, subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{3. Fragmented Multi-Vendor Instrumentation.} Our code review agent's execution spans multiple administrative domains: LLM serving (OpenAI API), agent orchestration (LangChain), vector storage (Pinecone), tool execution (subprocess calls), and inter-agent communication (custom APIs). Each layer has its own SDK, logging format, and instrumentation requirements, creating incompatible telemetry silos. When the security agent coordination fails, debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{How AI agent observability differs from classic software observability}
  \label{tab:diff}
  \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}p{2.85cm}X X@{}}
    \toprule
    \textbf{Dimension} &
    \textbf{Traditional app / micro-service} &
    \textbf{LLM or multi-agent system} \\
    \midrule
    What you try to “see” &
    Latency, errors, CPU, GC, SQL counts, request paths &
    \emph{Semantics}—prompt / tool trace, reasoning steps, toxicity, hallucination rate, persona drift, tokens/\$ spent \\
    Ground truth &
    Deterministic spec: given~$X$ produce~$Y$ or raise exception &
    Open-ended output: many acceptable~$Y$; quality judged by similarity, helpfulness, policy compliance \\
    Failure modes &
    Crashes, 5xx, memory leaks, deadlocks &
    Wrong facts, infinite loops, forgotten instructions, emergent mis-coordination between agents \\
    Time scale &
    Millisecond spans; state usually dies at request end &
    Dialogue and scratch memories persist for hours/days; “state’’ hides in vector DB rows and system prompts \\
    Signal source &
    Structured logs and metrics emitted on purpose &
    Often \emph{inside plain-text TLS payloads}; plus tool-execution logs \\
    Fix workflow &
    Reproduce, attach debugger, patch code &
    Re-prompt, fine-tune, change tool wiring, tweak guardrails—code may be fine but the “thought process’’ is wrong \\
    Safety / audit &
    Trace shows what code ran &
    Need evidence of \emph{why} the model said something for compliance / incident reviews \\
    \bottomrule
  \end{tabularx}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics.} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures—crashes, 5xx errors, memory leaks, latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality—capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary.} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior.} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance—infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable.} Returning to our code review agent: suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior—for example, writing a bash script with exploit commands (not logged as harmful file I/O) and then executing it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent.} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability lacks semantic primitives: attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed LLM/AI agent observability tooling as of early 2025, focusing on production-ready solutions that (a) provide SDKs, proxies, or specifications for integration with agent frameworks, and (b) offer telemetry capture, evaluation, or monitoring capabilities for LLM-based systems. Table 2 summarizes 12 representative industrial and open-source solutions, categorized by integration approach, capabilities, and licensing.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: Convergence and Critical Gaps}

Analysis of the surveyed tools reveals both promising standardization trends and fundamental architectural limitations:

\textbf{Finding 1: Universal Dependence on Application-Layer Instrumentation.} All 12 tools rely on application-level integration—SDK wrappers, middleware, or HTTP proxies. This creates three problems. First, maintenance burden: each framework update requires instrumentation updates. Second, evasion: dynamically-spawned tools bypass wrappers. Third, tampering: compromised agents can falsify logs. In our code review example, if the agent writes a malicious bash script (logged as file I/O) and executes it later (normal tool call), SDK instrumentation sees two unrelated events, missing the attack pattern.

\textbf{Finding 2: OpenTelemetry Convergence with Immature Semantics.} Five tools (Traceloop, Honeycomb, Langfuse, PromptLayer, Phoenix) have adopted OpenTelemetry (OTel)~\cite{Liu2025OTel}, suggesting standardization momentum. However, agent-specific attributes like \texttt{agent.goal}, \texttt{reasoning.loop\_id}, or \texttt{tool.coordination\_pattern} lack standardized definitions, forcing proprietary extensions that fragment the ecosystem despite shared transport protocols.

\textbf{Finding 3: Nascent Semantic Understanding.} Only four tools (Phoenix, LangSmith, Langfuse, Literal AI) provide LLM-powered semantic evaluation—toxicity detection, hallucination scoring. Most focus on operational metrics (latency, token cost). Critically, none address the \emph{why} question: given our code review agent's trace showing security agent coordination → linter execution → feedback posting, what was the reasoning chain? Which prompt evolution led to this sequence? Current tools capture event timelines but not causal intent.

\textbf{Finding 4: Complete Absence of System-Level Visibility.} No tool captures telemetry at kernel or network boundaries. None observe \texttt{execve()} syscalls, encrypted TLS payloads, or GPU driver interactions directly. When our code review agent spawns subprocess tools, SDK instrumentation sees only high-level API calls ("run linter"), not actual command arguments, environment variables, or subprocess network connections—details that remain immutable regardless of agent code changes.

\subsection{The Inescapable Conclusion: The Need for System-Level Observability}

Because current solutions live \emph{inside} the agent process, they inherit its fragility. Each workflow change requires redeploying instrumentation, causing breakage on prompt evolution. Prompt injection can silence or falsify logs, enabling evasion by compromised agents. Most critically, these approaches remain blind to cross-process interactions, unable to observe shell script execution, subprocess coordination, or external tool invocation.

System-level tracing at kernel and network boundaries addresses these gaps. Table 3 illustrates where SDK-based instrumentation fails and how boundary tracing provides complementary visibility:

\begin{table}[h]
\centering
\small
\begin{tabular}{p{0.45\columnwidth} p{0.45\columnwidth}}
\toprule
\textbf{Where today's SDKs stop} & \textbf{What boundary tracing sees} \\
\midrule
Missing span when agent spawns \texttt{curl} directly & \texttt{execve("curl", ...)} + network write \\
Agent mutates its own prompt string before logging & Raw ciphertext leaving the TLS socket \\
Sub-process mis-uses GPU & \texttt{ioctl} + CUDA driver calls \\
\bottomrule
\end{tabular}
\end{table}

Existing tools solve the "what happened inside my code?" question; system-level tracing answers "what actually hit the wire and the OS?"—a complementary, harder-to-tamper vantage point. This gap represents a critical research opportunity and motivates the architectural vision we present in Section 3.

\section{A Vision for a New Architecture: The Two-Plane Model}

Given the deployment challenges (security, cost, fragmentation) and the inadequacy of current solutions, we propose a fundamental architectural shift. Scalable agent observability requires two integrated layers:

\subsection{The Data Plane: Zero-Instrumentation Foundation}

\textbf{Core Principle:} Observability must be \emph{decoupled from agent internals}. Rather than instrumenting rapidly-evolving application code, capture telemetry at \emph{stable system boundaries}—kernel syscalls, network interfaces, and TLS endpoints.

\textbf{The Agent Stack Architecture.} Understanding where to observe requires understanding the agent architecture as three nested layers:


\begin{comment}
    

\begin{center}
    
\begin{Verbatim}[fontsize=\small, commandchars=\\\{\}]
┌───────────────────────────────────────────────┐
│          ☁  Rest of workspace / system       │
│  (APIs, DBs, message bus, OS, Kubernetes…)    │
│                                               │
│   ┌───────────────────────────────────────┐   │
│   │       Agent runtime / framework       │   │
│   │ (LangChain, claude-code, gemini-cli …)│   │
│   │  • orchestrates prompts & tool calls  │   │
│   │  • owns scratch memory / vector DB    │   │
│   └───────────────────────────────────────┘   │
│               ↑ outbound API calls            │
│───────────────────────────────────────────────│
│               ↓ inbound events                │
│   ┌───────────────────────────────────────┐   │
│   │          LLM serving provider         │   │
│   │    (OpenAI endpoint, local llama.cpp) │   │
│   └───────────────────────────────────────┘   │
└───────────────────────────────────────────────┘
\end{Verbatim}
\end{center}
\end{comment}


\begin{itemize}
  \item \textbf{LLM serving provider} – token generation, non-deterministic reasoning, chain-of-thought text that may or may not be surfaced. Most performance optimization work focuses on this layer.
  \item \textbf{Agent runtime layer} – orchestrates tasks into sequences of LLM calls plus external tool invocations; manages transient ``memories'' and state.
  \item \textbf{Outside world} – OS, containers, APIs, databases, other services and agents.
\end{itemize}

\textbf{The Observability Boundaries.} For observability purposes, the critical interfaces exist at three levels: the network boundary, where TLS-encrypted JSON inference requests and responses flow between agent and LLM provider; the system boundary, where syscalls and subprocess creation occur as agents execute tools (\texttt{curl}, \texttt{grep}, file I/O); and the inter-agent boundary, where message passing, shared memory, or API calls coordinate multiple agents.

Traditional software observability is \textbf{instrumentation-first}—manually inserting logs, spans, and metrics into application code. But AI agents \emph{dynamically modify their own logic} through prompt evolution, spontaneous tool creation, and self-modification. This constant internal mutability makes instrumentation fundamentally fragile.

\textbf{Benefits of System-Boundary Observability.} This approach offers framework neutrality, working across all agent runtimes (LangChain, AutoGen, gemini-cli, custom frameworks) without modification. It provides semantic stability by capturing prompt-level semantics without chasing rapidly-changing framework APIs, while maintaining trust and auditability through an independent audit trail that cannot be compromised by in-agent malware or prompt injection. Perhaps most critically, it enables construction of a universal causal graph that merges agent-level semantics (prompts, reasoning) with OS-level events (syscalls, network) into one coherent timeline, providing a single observability layer across the fragmented multi-vendor stack (LLM SaaS, agent code, system runtime, tools).

\textbf{Technical Implementation.} Technologies like eBPF~\cite{brendangregg,ebpfio} enable this vision by providing kernel-level tracing with <3\% overhead~\cite{zheng2025extending}. Key capabilities include TLS interception through hooking userspace TLS read/write functions to capture encrypted LLM traffic without proxies, syscall tracing to monitor \texttt{execve()}, file I/O, and network calls for tracking tool execution, and process genealogy tracking to understand agent-spawned workflow chains.

\subsection{The Cognitive Plane: Agents Observing Agents}

\textbf{The Human Scalability Crisis.} Even with perfect telemetry from the Data Plane, human operators cannot effectively monitor thousands of production agents generating millions of events per hour. The sheer volume, combined with the semantic complexity (understanding reasoning chains, detecting subtle persona drift, correlating multi-agent failures), exceeds human cognitive capacity.

\textbf{Core Thesis:} The only way to observe complex, autonomous AI systems at scale is with \emph{other autonomous AI systems}. We propose a Cognitive Plane—an autonomous multi-agent system dedicated to observability itself.

\textbf{Architecture.} The Cognitive Plane consists of specialized observability agents~\cite{Rombaut2025Watson,Kim2025AgenticInterp} organized in a coordinated hierarchy. Observer agents continuously monitor telemetry streams from the Data Plane, applying learned patterns to detect anomalies such as reasoning loops, cost spikes, and security violations~\cite{Dong2024AgentOps}. Diagnoser agents perform automated root-cause analysis by correlating semantic traces (prompt evolution) with system events (syscalls, network failures), using LLM reasoning to generate human-readable incident reports explaining \emph{why} a failure occurred~\cite{Moshkovich2025Pipeline}. Remediator agents take autonomous corrective actions within defined safety boundaries—circuit-breaking runaway agents, adjusting prompt temperature, scaling resources—while escalating high-risk decisions to human operators via structured recommendations. Finally, cost optimizer agents analyze token usage, API call patterns, and reasoning efficiency across all agents to identify optimization opportunities and automatically implement approved changes.

\textbf{Key Properties.} This architecture exhibits three critical properties that distinguish it from traditional monitoring. First, LLM-powered agents provide semantic understanding, interpreting natural language prompts, reasoning chains, and tool interactions to bridge the semantic gap that rule-based systems cannot cross. Second, observability agents engage in continuous learning from historical incidents, improving detection accuracy and expanding their understanding of normal versus anomalous behavior over time. Third, the Cognitive Plane enables proactive management rather than reactive debugging, predicting failures (e.g., detecting early signs of reasoning loops) and intervening before user impact occurs.

\section{Future Directions and Open Challenges}

\textbf{1. Causal Observability.} Current systems correlate events; future work must establish \emph{causal relationships}. Why did an agent make a specific decision? What would have happened with a different prompt? This requires causal inference techniques applied to agent telemetry.

\textbf{2. Evaluation Benchmarks.} The field lacks standardized benchmarks for agent observability systems. We need datasets of real agent failures, metrics for semantic coverage, and evaluation frameworks for observability agent effectiveness.

\textbf{3. Security of the Observability Layer.} If the Cognitive Plane is compromised, attackers gain visibility into all production agents. Research is needed on tamper-proof telemetry channels, secure multi-party computation for sensitive traces, and Byzantine-fault-tolerant observability architectures.

\textbf{4. The Semantic Gap.} While system-level tracing captures raw events, extracting agent \emph{intent} requires sophisticated NLP and program synthesis. How do we infer "the agent is trying to accomplish X" from syscall sequences and TLS payloads?

\textbf{5. Standardization.} OpenTelemetry semantic conventions for agents are nascent. The community needs consensus on span attributes (\texttt{agent.reasoning.loop\_id}, \texttt{tool.coordination.pattern}), trace topology (how to represent multi-agent interactions), and evaluation schemas.

\section{Conclusion}

The current fragmented, model-centric approach to AI agent observability represents a fundamental barrier to large-scale deployment. Application-level instrumentation creates brittle SDK silos that cannot address the security, cost, and complexity challenges inherent to agent systems. Traditional APM tools, LLM-centric monitoring, and serving observability each address only narrow slices of the problem, leaving critical blind spots.

We argue that the future of reliable, large-scale agentic AI depends on adopting a unified, two-plane observability architecture: (1) a \textbf{Data Plane} that provides zero-instrumentation telemetry capture at stable system boundaries, decoupling observability from rapidly-evolving agent internals while providing tamper-resistant audit trails, and (2) a \textbf{Cognitive Plane} where autonomous observability agents provide the semantic understanding and proactive management that human operators cannot achieve at scale.

This architectural vision directly addresses the deployment challenges we identified: system-boundary observability eliminates SDK fragmentation and security vulnerabilities; unified telemetry enables cross-layer cost optimization; and autonomous observability agents bridge the semantic gap between raw events and agent intent. Most importantly, this approach scales: as agent deployments grow from hundreds to millions, the observability system grows with them—not through human labor, but through autonomous agent-to-agent coordination.

The path forward requires sustained research effort across the community: developing causal inference techniques for agent behavior, establishing evaluation benchmarks, securing the observability infrastructure itself, and standardizing semantic conventions. But the fundamental insight is clear: \textbf{only agents can truly observe agents at scale}. The question is not whether this transition will occur, but how quickly we can build the foundations to make it possible.

\bibliographystyle{ACM-Reference-Format}
\bibliography{../ai}

\end{document}

