
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment
\usepackage{fontspec}   % For custom font symbols

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line
\newunicodechar{☁}{{\fontspec{Symbola}\symbol{"2601}}} % Cloud symbol

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
}


\sloppy
\begin{abstract}
AI agent systems are transforming software infrastructure through autonomous task orchestration, but their production deployment reveals a fundamental mismatch: traditional observability assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving internal logic. Current solutions—traditional APM, model-centric LLM monitoring, and framework-specific instrumentation—create fragmented SDK silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. We propose a two-plane architecture: (1) a Data Plane providing zero-instrumentation telemetry capture at stable system boundaries (kernel, network, TLS), decoupling observability from agent internals, and (2) a Cognitive Plane where autonomous observability agents provide semantic understanding and proactive management that human operators cannot achieve at scale. This approach addresses the unique demands of agent systems while enabling secure, cost-effective, and semantically-aware observability.
\end{abstract>


\maketitle



\section{Introduction: The Agent Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows: debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state—all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{1. Semantic Failures Replace Deterministic Errors.} In our code review example, the agent might hallucinate a security vulnerability that doesn't exist, enter an infinite loop requesting more context, or forget critical style guidelines mid-review. Unlike traditional crashes (segfaults, exceptions), these failures are \emph{semantic}—plausible but incorrect outputs that require understanding agent \emph{intent} to detect. More critically, prompt injection attacks~\cite{indirect-prompt-inject} can compromise agents to evade their own logging, hiding malicious behavior. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{2. Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer: token usage for LLM calls, vector database queries, API calls to GitHub, subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{3. Fragmented Multi-Vendor Instrumentation.} Our code review agent's execution spans multiple administrative domains: LLM serving (OpenAI API), agent orchestration (LangChain), vector storage (Pinecone), tool execution (subprocess calls), and inter-agent communication (custom APIs). Each layer has its own SDK, logging format, and instrumentation requirements, creating incompatible telemetry silos. When the security agent coordination fails, debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{How AI agent observability differs from classic software observability}
  \label{tab:diff}
  \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}p{2.85cm}X X@{}}
    \toprule
    \textbf{Dimension} &
    \textbf{Traditional app / micro-service} &
    \textbf{LLM or multi-agent system} \\
    \midrule
    What you try to “see” &
    Latency, errors, CPU, GC, SQL counts, request paths &
    \emph{Semantics}—prompt / tool trace, reasoning steps, toxicity, hallucination rate, persona drift, tokens/\$ spent \\
    Ground truth &
    Deterministic spec: given~$X$ produce~$Y$ or raise exception &
    Open-ended output: many acceptable~$Y$; quality judged by similarity, helpfulness, policy compliance \\
    Failure modes &
    Crashes, 5xx, memory leaks, deadlocks &
    Wrong facts, infinite loops, forgotten instructions, emergent mis-coordination between agents \\
    Time scale &
    Millisecond spans; state usually dies at request end &
    Dialogue and scratch memories persist for hours/days; “state’’ hides in vector DB rows and system prompts \\
    Signal source &
    Structured logs and metrics emitted on purpose &
    Often \emph{inside plain-text TLS payloads}; plus tool-execution logs \\
    Fix workflow &
    Reproduce, attach debugger, patch code &
    Re-prompt, fine-tune, change tool wiring, tweak guardrails—code may be fine but the “thought process’’ is wrong \\
    Safety / audit &
    Trace shows what code ran &
    Need evidence of \emph{why} the model said something for compliance / incident reviews \\
    \bottomrule
  \end{tabularx}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics.} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures—crashes, 5xx errors, memory leaks, latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality—capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary.} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior.} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance—infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable.} Returning to our code review agent: suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior—for example, writing a bash script with exploit commands (not logged as harmful file I/O) and then executing it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent.} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability lacks semantic primitives: attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed LLM/AI agent observability tooling as of early 2025, focusing on production-ready solutions that (a) provide SDKs, proxies, or specifications for integration with agent frameworks, and (b) offer telemetry capture, evaluation, or monitoring capabilities for LLM-based systems. Table 2 summarizes 12 representative industrial and open-source solutions, categorized by integration approach, capabilities, and licensing.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: Convergence and Critical Gaps}

Analysis of the surveyed tools reveals both promising standardization trends and fundamental architectural limitations:

\textbf{Finding 1: Universal Dependence on Application-Layer Instrumentation.} All 12 tools rely on application-level integration—SDK wrappers, middleware, or HTTP proxies. This creates three problems. First, maintenance burden: each framework update requires instrumentation updates. Second, evasion: dynamically-spawned tools bypass wrappers. Third, tampering: compromised agents can falsify logs. In our code review example, if the agent writes a malicious bash script (logged as file I/O) and executes it later (normal tool call), SDK instrumentation sees two unrelated events, missing the attack pattern.

\textbf{Finding 2: OpenTelemetry Convergence with Immature Semantics.} Five tools (Traceloop, Honeycomb, Langfuse, PromptLayer, Phoenix) have adopted OpenTelemetry (OTel)~\cite{Liu2025OTel}, suggesting standardization momentum. However, agent-specific attributes like \texttt{agent.goal}, \texttt{reasoning.loop\_id}, or \texttt{tool.coordination\_pattern} lack standardized definitions, forcing proprietary extensions that fragment the ecosystem despite shared transport protocols.

\textbf{Finding 3: Nascent Semantic Understanding.} Only four tools (Phoenix, LangSmith, Langfuse, Literal AI) provide LLM-powered semantic evaluation—toxicity detection, hallucination scoring. Most focus on operational metrics (latency, token cost). Critically, none address the \emph{why} question: given our code review agent's trace showing security agent coordination → linter execution → feedback posting, what was the reasoning chain? Which prompt evolution led to this sequence? Current tools capture event timelines but not causal intent.

\textbf{Finding 4: Complete Absence of System-Level Visibility.} No tool captures telemetry at kernel or network boundaries. None observe \texttt{execve()} syscalls, encrypted TLS payloads, or GPU driver interactions directly. When our code review agent spawns subprocess tools, SDK instrumentation sees only high-level API calls ("run linter"), not actual command arguments, environment variables, or subprocess network connections—details that remain immutable regardless of agent code changes.

\subsection{Implications: The Case for System-Boundary Observability}

The surveyed tools address the question "what happened inside my code?" but cannot answer "what actually reached the OS and network?"—a complementary perspective that is both tamper-resistant and framework-agnostic. Table 3 illustrates where SDK instrumentation fails and how system-boundary tracing provides complementary visibility:

\begin{table}[h]
\centering
\small
\begin{tabular}{p{0.45\columnwidth} p{0.45\columnwidth}}
\toprule
\textbf{Where SDK instrumentation stops} & \textbf{What boundary tracing captures} \\
\midrule
Missing span when agent spawns \texttt{curl} directly & \texttt{execve("curl", ...)} + network write \\
Agent mutates prompt before logging & Raw TLS payload leaving socket \\
Subprocess GPU misuse & \texttt{ioctl} + CUDA driver calls \\
\bottomrule
\end{tabular}
\end{table}

This analysis motivates a fundamental architectural shift: observability must move from fragile in-process instrumentation to stable system boundaries. The next section presents our vision for achieving this through a two-plane architecture that addresses both the instrumentation gap (via system-boundary capture) and the semantic gap (via autonomous observability agents).

\section{A Two-Plane Architecture for Agent Observability}

Having established that current solutions cannot bridge the instrumentation and semantic gaps, we now present an architectural vision that addresses both challenges through complementary layers: a Data Plane that captures tamper-resistant telemetry at system boundaries, and a Cognitive Plane where autonomous agents provide semantic understanding at scale.

\subsection{The Data Plane: Zero-Instrumentation Foundation}

\textbf{Core Principle:} Observability must be \emph{decoupled from agent internals}. Rather than instrumenting rapidly-evolving application code, capture telemetry at \emph{stable system boundaries}—kernel syscalls, network interfaces, and TLS endpoints.

\textbf{The Agent Stack.} Agent systems comprise three nested layers: (1) \emph{LLM serving provider}—token generation and chain-of-thought reasoning (OpenAI API, local llama.cpp); (2) \emph{Agent runtime}—orchestration logic coordinating LLM calls, tool invocations, and state management (LangChain, AutoGen, Claude Code); (3) \emph{External environment}—OS, containers, APIs, databases, other agents.

Returning to our code review agent: when it queries the vector database for similar bugs, the agent runtime (LangChain) makes an HTTPS call to Pinecone's API. When it spawns \texttt{pylint}, the runtime executes \texttt{fork()} + \texttt{execve()} syscalls. When it coordinates with the security agent, it sends JSON over a network socket. These interactions cross system boundaries where observability can capture immutable telemetry.

\textbf{The Three Observability Boundaries.} The Data Plane captures telemetry at three stable interfaces: (1) \emph{Network boundary}—TLS-encrypted JSON requests/responses between agent runtime and LLM provider, revealing prompt content, token usage, and model parameters; (2) \emph{System boundary}—syscalls (\texttt{execve()}, \texttt{open()}, \texttt{connect()}) showing tool execution, file access, and subprocess spawning; (3) \emph{Inter-agent boundary}—API calls, message passing, or shared memory coordination between multiple agents.

\textbf{Why This Approach Works.} System-boundary observability provides four critical properties. \emph{Framework neutrality}: works across LangChain, AutoGen, custom frameworks without code changes—our code review agent remains observable even if rewritten from LangChain to raw Python. \emph{Tamper resistance}: compromised agents cannot falsify kernel syscall traces or TLS socket writes—the malicious bash script attack becomes visible as \texttt{write("exploit.sh")} + \texttt{execve("exploit.sh")}. \emph{Semantic richness}: TLS interception captures full prompt text, tool arguments, and reasoning chains without instrumenting agent code. \emph{Unified causality}: correlates agent semantics (prompts) with system events (subprocess execution) in one timeline, eliminating multi-vendor telemetry silos.

\textbf{Technical Feasibility.} Technologies like eBPF~\cite{brendangregg,ebpfio} enable kernel-level tracing with <3\% overhead~\cite{zheng2025extending}. Userspace TLS function hooking (OpenSSL \texttt{SSL\_read/write}, Go \texttt{crypto/tls}) captures encrypted LLM traffic without proxies. Process genealogy tracking reconstructs agent-spawned subprocess trees. The Data Plane thus provides comprehensive, tamper-resistant telemetry decoupled from agent internals.

\subsection{The Cognitive Plane: Agents Observing Agents}

While the Data Plane solves the instrumentation gap, it cannot bridge the semantic gap alone. Raw syscall traces and TLS payloads do not explain \emph{why} our code review agent spawned \texttt{pylint} at this moment, whether this action aligns with its goal, or if the reasoning chain exhibits early signs of drift. Semantic understanding requires intelligence—and at production scale (thousands of agents, millions of events per hour), only autonomous systems can provide it.

\textbf{Core Principle:} The only scalable way to observe autonomous AI systems is with \emph{other autonomous AI systems}. We propose a Cognitive Plane—a coordinated multi-agent system dedicated to observability itself.

\textbf{Architecture: Specialized Observability Agents.} The Cognitive Plane comprises four agent types~\cite{Rombaut2025Watson,Kim2025AgenticInterp}, each addressing distinct observability challenges:

\emph{Observer agents} continuously monitor Data Plane telemetry streams, applying learned patterns to detect anomalies: reasoning loops (the code review agent requests context five times in succession), cost spikes (unexpected token usage surge), security violations (subprocess writes to \texttt{/etc/passwd})~\cite{Dong2024AgentOps}. When our code review agent's token usage suddenly doubles, the observer flags the anomaly for diagnosis.

\emph{Diagnoser agents} perform root-cause analysis by correlating semantic traces (prompt evolution, tool call sequences) with system events (syscall patterns, network failures). Using LLM reasoning, they generate human-readable incident reports: "Agent entered reasoning loop because vector database query returned no results, triggering repeated context requests"~\cite{Moshkovich2025Pipeline}. This bridges raw telemetry to causal explanation.

\emph{Remediator agents} take autonomous corrective actions within safety boundaries: circuit-breaking runaway agents, adjusting prompt temperature, scaling vector database resources. High-risk actions (modifying production agent prompts) escalate to human operators with structured recommendations. When the observer detects a cost spike, the remediator automatically reduces \texttt{temperature} to 0.3 and notifies the operator.

\emph{Cost optimizer agents} analyze token usage patterns, API call efficiency, and tool coordination across all agents, identifying optimization opportunities: "Replace sequential linter calls with parallel subprocess pool" or "Cache vector database results for 5 minutes." Approved optimizations are automatically deployed.

\textbf{Key Properties.} This architecture provides three capabilities traditional monitoring cannot achieve. \emph{Semantic understanding}: LLM-powered agents interpret natural language prompts, reasoning quality, and multi-step coordination—bridging the semantic gap. \emph{Continuous learning}: observability agents improve from historical incidents, expanding their models of normal versus anomalous behavior. \emph{Proactive management}: rather than reactive debugging, the Cognitive Plane predicts failures (detecting early reasoning loop patterns) and intervenes before user impact.

\section{Open Research Challenges}

Realizing this vision requires sustained research effort across five critical dimensions:

\textbf{Causal Inference for Agent Behavior.} Current systems correlate events but cannot answer counterfactuals: Why did our code review agent choose \texttt{pylint} over \texttt{flake8}? What would have happened with temperature 0.7 instead of 0.3? Causal observability requires techniques from causal inference applied to agent telemetry—identifying decision points, instrumenting counterfactual branches, and constructing causal graphs from observational data.

\textbf{Benchmarks and Evaluation.} The field lacks standardized evaluation frameworks. We need: (1) datasets of real agent failures with ground-truth root causes, (2) metrics for semantic coverage (what fraction of reasoning intent is captured?), (3) adversarial test suites for tamper resistance, and (4) evaluation protocols for Cognitive Plane effectiveness (detection latency, false positive rates, remediation success).

\textbf{Securing the Observability Infrastructure.} If the Cognitive Plane is compromised, attackers gain visibility into all production agents—telemetry data, prompts, coordination patterns. Critical research questions include: How do we build tamper-proof telemetry channels from Data Plane to Cognitive Plane? Can secure multi-party computation protect sensitive traces while enabling collaborative diagnosis? How do we design Byzantine-fault-tolerant observability agents that function correctly even when some agents are malicious?

\textbf{Semantic Reconstruction from System Events.} While eBPF captures syscalls and TLS payloads, inferring high-level agent \emph{intent} requires bridging a semantic abstraction gap. When we observe \texttt{execve("pylint")} followed by network writes to GitHub API, how do we reconstruct the reasoning: "Agent determined code violated style guide, ran linter to generate specific examples, posted structured feedback"? This demands program synthesis, NLP, and machine learning techniques to map low-level events to high-level goals.

\textbf{Standardization of Semantic Conventions.} OpenTelemetry for agents remains nascent. Community consensus is needed on: span attributes for agent semantics (\texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}), trace topology for multi-agent interactions (how to represent coordination, delegation, conflict), evaluation result schemas (hallucination scores, relevance metrics), and unified cost attribution across LLM, tool, and infrastructure layers.

\section{Conclusion}

Production deployment of AI agent systems faces a fundamental observability challenge: traditional approaches assume deterministic code with stable interfaces, but agents generate non-deterministic outputs through rapidly-evolving internal logic. Our analysis of current solutions reveals universal dependence on application-layer instrumentation that creates three critical problems—maintenance burden from framework churn, evasion by dynamically-spawned tools, and tampering vulnerabilities when agents are compromised. These limitations stem from two complementary gaps: the instrumentation gap (agent code is unstable and tamperable) and the semantic gap (system events lack intent).

We propose a two-plane architecture that addresses both challenges. The \textbf{Data Plane} captures tamper-resistant telemetry at stable system boundaries—kernel syscalls, network interfaces, TLS endpoints—providing framework-neutral observability decoupled from agent internals. Technologies like eBPF enable comprehensive tracing of subprocess execution, encrypted LLM traffic, and inter-agent coordination with minimal overhead. The \textbf{Cognitive Plane} deploys autonomous observability agents that provide semantic understanding at scale: observers detect anomalies in reasoning patterns, diagnosers correlate telemetry to explain root causes, remediators take corrective actions within safety boundaries, and optimizers identify efficiency improvements. Together, these planes bridge raw system events to causal intent, enabling unified observability across the fragmented multi-vendor agent stack.

This approach directly addresses the deployment challenges motivating our work. System-boundary observability eliminates SDK fragmentation and provides tamper-resistant audit trails immune to prompt injection. Unified telemetry enables cross-layer cost optimization and security enforcement. Autonomous observability agents scale with agent deployments—from hundreds to millions—through self-coordination rather than human labor. Most importantly, this architecture acknowledges a fundamental reality: \textbf{only autonomous systems can effectively observe autonomous systems at production scale}.

Realizing this vision requires sustained community effort: developing causal inference techniques, establishing evaluation benchmarks, securing the observability infrastructure itself, standardizing semantic conventions. But the path forward is clear. As agent systems transform software infrastructure, observability must evolve from fragile in-process instrumentation to autonomous, boundary-based monitoring. The future of reliable agentic AI depends on building these foundations now.

\bibliographystyle{ACM-Reference-Format}
\bibliography{../ai}

\end{document}

