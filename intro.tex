\section{Introduction}

The role of machine learning in systems is undergoing a fundamental shift from optimizing well-defined tasks, such as database query planning, to a new paradigm of \emph{agentic computing}. From a systems perspective, an AI agent couples a Large Language Model's (LLM) reasoning with direct access to system tools, granting it agency to perform operations like spawning processes, modifying the filesystem, and executing commands. This technology is being rapidly integrated into production environments, powering autonomous developer tools like Claude Code\cite{claudecode}, Cursor Agent\cite{cursor} and Gemini-CLI\cite{geminicli}, which can independently handle complex software engineering and system maintenance tasks. In essence, we are deploying non-deterministic ML systems, creating an unprecedented class of challenges for system reliability, security, and verification.

This paradigm shift creates a critical semantic gap: the chasm between an agent's high-level \emph{intent} and its low-level \emph{system actions}. Unlike traditional programs with predictable execution paths, agents use LLMs and autonomous tools to dynamically generate code and spawn arbitrary subprocesses. This makes it hard for existing observability tools to distinguish benign operations from catastrophic failures. Consider an agent tasked with code refactoring that, due to a malicious prompt it reads from external url in the search result when search for API documents, instead injects a backdoor (indirect prompt injection)\cite{indirect-prompt-inject}. An application-level monitor might see a successful "execute script" tool call, while a system monitor sees a \texttt{bash} process writing to a file. Neither can bridge the gap to understand that a benign intention has been twisted into a malicious action, rendering them effectively blind.

Current approaches are trapped on one side of this semantic gap. \emph{Application-level instrumentation}, found in frameworks like LangChain~\cite{langchain} and AutoGen~\cite{autogen}, captures an agent's reasoning and tool selection. While these tools see the \emph{intent}, they are brittle, require constant API updates, and are easily bypassed: a single shell command escapes their view, breaking the chain of visibility under a flawed trust model. Conversely, \emph{generic system-level monitoring} sees the \emph{actions}, tracking every system call and file access. However, it lacks all semantic context. To such a tool, an agent writing a data analysis script is indistinguishable from a compromised agent writing a malicious payload. Without understanding the preceding LLM instructions, the \emph{why} behind the \emph{what}, its stream of low-level events is meaningless noise.

We propose {boundary tracing} as a novel observability method designed specifically to bridge this semantic gap. Our key insight is that while agent internals and frameworks are volatile, the interfaces through which they interact with the world (the kernel for system operations and the network for communication) are stable and unavoidable. By monitoring from outside the application at these boundaries, we can capture an agent's high-level intent and its low-level system effects. We present \textbf{AgentSight}, a system that realizes boundary tracing using eBPF to intercept TLS-encrypted LLM traffic for intent and monitor kernel events for effects. Its core is a novel, two-stage correlation process: a real-time engine links an LLM response to the system bahavior it triggers, and a secondary "observer" LLM performs a deep semantic analysis on the resulting trace to infer risks and explain \emph{why} a sequence of events is suspicious. This instrumentation-free, framework-agnostic technique incurs less than 3\% overhead and effectively detects prompt injection attacks, resource-wasting reasoning loops, and multi-agent system bottlenecks.

In summary, our contributions are:

\begin{enumerate}
\item We introduce boundary tracing as a principled approach to AI agent observability that bridges the semantic gap by monitoring at stable system interfaces.
\item We present a novel engine that combines real-time, eBPF-based signal matching with LLM-based semantic analysis to provide deep, contextual understanding of agent behavior.
\item We demonstrate AgentSight's effectiveness in detecting prompt injection attacks, reasoning loops, and multi-agent coordination failures with sub-3\% overhead.
\end{enumerate}

% This semantic gap makes it impossible for existing observability tools to distinguish between benign operations and catastrophic failures. Current approaches fall into two categories, each blind to one side of the gap. {Application-level instrumentation}, found in frameworks like LangChain~\cite{langchain} and AutoGen~\cite{autogen}, uses hooks and logs to capture the agent's reasoning and tool selections. While these tools see the \emph{intent}, they are fundamentally limited. They are brittle, requiring constant updates to keep pace with framework APIs that see dozens of breaking changes monthly. More critically, they are easily bypassed; a single shell command spawned by an agent escapes their view, breaking the chain of visibility. They operate on a flawed trust model, assuming a cooperative agent that will not be compromised or exhibit emergent, unlogged behaviors.

% On the other side, {generic system-level monitoring} tools see the \emph{actions}. They can track every system call, file access, and network connection. However, they lack all semantic context. To them, an agent writing a data analysis script to \texttt{/tmp} is indistinguishable from a compromised agent writing a malicious payload to the same location. Without understanding the preceding LLM instructions, the \emph{why} behind the \emph{what}, their stream of low-level events is meaningless noise, leading to an overwhelming volume of false positives and inactionable alerts.