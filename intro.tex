\section{Introduction}

The rapid deployment of AI agents in production environments has created an unprecedented observability crisis. These systems, which combine Large Language Models (LLMs) with autonomous tool use capabilities, fundamentally violate the assumptions underlying traditional software monitoring. Unlike deterministic programs with predefined execution paths, AI agents dynamically construct execution plans, spawn arbitrary subprocesses, and modify their behavior based on natural language objectives. Consider a production incident where an AI agent, tasked with analyzing customer data, autonomously installed Python packages, executed data processing scripts in subprocesses, and uploaded results to external servers—all while traditional monitoring remained blind to these critical operations occurring outside instrumented framework boundaries.

Current observability approaches for AI agents rely predominantly on application-level instrumentation within frameworks such as LangChain~\cite{langchain}, AutoGen~\cite{autogen}, and Claude Code. These solutions implement callback hooks, middleware layers, or explicit logging statements embedded within agent logic. However, this strategy suffers from fundamental limitations. Agent frameworks undergo rapid evolution with frequent breaking changes—LangChain alone averaged multiple API modifications per month throughout 2024. More critically, agents possess the capability to execute operations that entirely bypass instrumentation. When an agent generates and executes shell scripts that spawn additional programs, framework-level monitoring loses visibility at each process boundary. The trust model of traditional instrumentation assumes cooperative behavior, yet agents can be compromised through prompt injection or exhibit emergent behaviors that disable logging or execute operations through uninstrumented channels.

We propose \emph{boundary tracing} as a novel observability paradigm for AI agents. Our key insight is that while agent internals evolve rapidly and unpredictably, the interfaces through which agents interact with their environment remain stable. All meaningful agent actions must traverse well-defined system boundaries: the kernel interface for system operations and the network interface for external communications. By observing at these boundaries rather than within agent code, we achieve stable, comprehensive monitoring independent of agent implementation details. This approach fundamentally shifts the trust model from assuming cooperative agents to enforcing observation at tamper-proof system boundaries.

This paper presents AgentSight, a system-level observability framework for AI agents that implements boundary tracing using eBPF (extended Berkeley Packet Filter) technology. AgentSight captures both semantic information through TLS interception of LLM communications and system behavior through comprehensive kernel monitoring, correlating these perspectives to understand agent behavior holistically. Our evaluation across diverse production workloads demonstrates that AgentSight maintains less than 3\% performance overhead while successfully detecting prompt injection attacks, identifying reasoning loops, and revealing coordination patterns in multi-agent systems that traditional monitoring approaches miss entirely.

\textbf{This paper makes three primary contributions:}

\begin{enumerate}
\item \textbf{Boundary Tracing Paradigm}: We introduce boundary tracing as a principled approach to AI agent observability that monitors at stable system interfaces, addressing fundamental limitations of application-level instrumentation.

\item \textbf{System-Level Implementation}: We present AgentSight's eBPF-based implementation that correlates TLS-intercepted LLM communications with kernel-level operations across process boundaries.

\item \textbf{Production Validation}: We demonstrate AgentSight's effectiveness in detecting prompt injection attacks (92\% confidence), reasoning loops, and multi-agent coordination failures with sub-3\% overhead.
\end{enumerate}