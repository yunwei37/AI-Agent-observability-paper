\section{Introduction}

Consider a scenario where an AI agent deployed in a production environment receives a seemingly innocuous request to analyze customer data. The agent autonomously decides to install new Python packages, writes a data processing script, executes it in a subprocess, and then uploads results to an external server. Traditional application monitoring captures none of these critical actions because they occur outside the instrumented framework boundaries. This gap between agent capabilities and observability infrastructure represents a fundamental security and reliability risk as organizations rapidly deploy AI agents in production systems.

AI agents—systems that combine LLMs with autonomous tool use—fundamentally differ from traditional software. They generate execution plans dynamically, spawn arbitrary subprocesses, and modify their behavior based on natural language objectives. This autonomy creates unprecedented observability challenges that existing monitoring approaches cannot address. Unlike deterministic programs with predefined execution paths, agents behave more like autonomous users, making decisions based on context and generating novel solutions to problems. This behavioral complexity means that understanding what an agent is doing requires capturing both its high-level reasoning process and its low-level system interactions.

Current approaches to agent observability rely heavily on application-level instrumentation within agent frameworks such as LangChain, AutoGen, and Claude Code. These solutions typically implement callback hooks, middleware layers, or explicit logging statements embedded within agent logic. However, this strategy faces critical limitations that become apparent in production deployments. Agent frameworks undergo rapid iteration cycles with frequent breaking changes—LangChain alone has averaged multiple API modifications per month throughout 2024. More critically, agents possess the capability to execute code that entirely bypasses instrumentation. When an agent writes and executes a shell script that launches additional programs, framework-level monitoring loses visibility at each boundary crossing. The trust model of traditional instrumentation also assumes cooperative behavior, yet agents can be influenced through prompt injection or emergent behaviors to disable logging or execute operations through uninstrumented channels.

We propose \emph{boundary tracing} as a novel approach to AI agent observability. The key insight is that while agent internals may change rapidly and unpredictably, the interfaces through which agents interact with their environment remain stable. All meaningful agent actions must traverse well-defined system boundaries: the kernel interface for system operations and the network interface for external communications. By observing at these boundaries rather than within agent code, we achieve stable, comprehensive monitoring independent of agent implementation details. This approach leverages the principle that system calls, network protocols, and file system operations provide consistent observation points that persist across framework versions and agent modifications.

This paper makes four key contributions to address the AI agent observability challenge. First, we introduce the boundary tracing concept as a principled approach to monitoring autonomous AI systems, demonstrating how observation at system interfaces provides more reliable visibility than application-level instrumentation. Second, we present AgentSight's implementation using eBPF technology, showing how to capture both semantic information through TLS interception and system behavior through kernel monitoring while maintaining less than 3\% performance overhead in production workloads. Third, we develop novel techniques for correlating high-level agent intentions captured at the network boundary with low-level system operations observed at the kernel boundary, enabling semantic understanding of agent behavior across abstraction levels. Fourth, we provide comprehensive case studies across multiple agent frameworks revealing previously unknown behavioral patterns, including prompt injection vulnerabilities, reasoning loops, and multi-agent coordination failures that traditional monitoring approaches miss entirely.

The remainder of this paper is organized as follows. Section 2 provides background on AI agent architectures and motivates the need for new observability approaches through concrete examples and a systematic comparison with traditional software monitoring. Section 3 surveys related work in agent observability, analyzing current commercial and open-source solutions while identifying critical gaps that motivate our approach. Section 4 presents the boundary tracing concept and AgentSight's design principles, explaining how system-level observation addresses the limitations of application-level instrumentation. Section 5 details our implementation using eBPF for both TLS interception and system call monitoring, including the streaming analysis framework for event correlation. Section 6 evaluates AgentSight across multiple dimensions, demonstrating performance characteristics and effectiveness through three detailed case studies. Section 7 discusses future research directions and open challenges in AI agent observability. Section 8 concludes with the implications of our work for the safe deployment of autonomous AI systems. We release AgentSight as open source at https://github.com/eunomia-bpf/agentsight to enable community research on this critical infrastructure challenge.