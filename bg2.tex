\section{Background and Related Work}

This section outlines LLM agent architecture, reviews existing observability work to highlight the semantic gap, and introduces eBPF as our foundational technology.

% \subsection{LLM Agent Architecture}
% AI agents represent a new class of software systems that combine language models with environmental interactions. These systems typically consist of three core components: (1) an LLM backend that provides reasoning capabilities, (2) a tool execution framework that enables system interactions, and (3) a control loop that orchestrates prompts, tool calls, and state management. Popular frameworks such as LangChain~\cite{langchain}, AutoGen~\cite{autogen}, cursor agent\cite{cursor}, genimi-cli\cite{geminicli} and Claude Code\cite{claudecode} implement variations of this architecture and are widely used in production. The key characteristic distinguishing AI agents from traditional software is their ability to dynamically construct execution plans based on natural language objectives (e.g., an agent tasked with "analyze this dataset" might autonomously decide to install packages, write analysis scripts, execute them, and interpret results), all without predetermined logic paths. This flexibility comes from the LLM's ability to generate arbitrary code and command sequences.

\subsection{LLM Agent Architecture}
The agentic systems described in the introduction are typically implemented using a common architecture. These systems consist of three core components: (1) an LLM backend for reasoning, (2) a tool execution framework for system interactions, and (3) a control loop that orchestrates prompts, tool calls, and state management. Popular frameworks such as LangChain~\cite{langchain}, AutoGen~\cite{autogen}, cursor agent\cite{cursor}, genimi-cli\cite{geminicli} and Claude Code\cite{claudecode} all implement variations of this model. This architecture is what enables agents to dynamically construct and execute complex plans (e.g., autonomously writing and running a script to analyze a dataset) based on high-level natural language objectives.

\subsection{Observability for LLM Agent}

Existing approaches are siloed on one side of the semantic gap. Intent-side observability, supported by industry tools like Langfuse, LangSmith, and Datadog~\cite{Maierhofer2025Langfuse, langfuse, langsmith, Datadog2023Agents, helicone} and is unifying by standards from the OpenTelemetry GenAI working group~\cite{Liu2025OTel,Bandurchin2025Uptrace} and acadamics conceptual taxonomies~\cite{Dong2024AgentOps, Moshkovich2025Pipeline} under the AgentOps concept, excels at tracing application-level events but is fundamentally blind to out-of-process system \emph{actions}. Conversely, action-side observability with tools like Falco and Tracee~\cite{falco, tracee} offers comprehensive visibility into system calls but lacks the semantic context to understand an agent's \emph{intent}, failing to distinguish a benign task from a malicious one. A parallel line of research into reasoning-level and interpretability aims to make the agent's internal thought processes more transparent by reconstructing cognitive traces~\cite{Rombaut2025Watson} or enabling explanatory dialogues~\cite{Kim2025AgenticInterp}, but these work mainly focus on the llm itself, does not bridge the gap between the agent's internal reasoning and its external, low-level effects on the system.

\subsection{extended Berkeley Packet Filter (eBPF)}

To bridge the semantic gap, our approach requires a technology that can safely and efficiently observe both network communication and kernel activity. eBPF (extended Berkeley Packet Filter) is a fundamental advancement in kernel programmability that provides precisely this capability~\cite{brendangregg}. Originally designed for packet filtering, eBPF has evolved into a general-purpose, in-kernel virtual machine that powers modern observability and security tools~\cite{ebpfio,cilium}. For AI agent observability, eBPF is uniquely suited because it allows observation at the exact boundaries where agents interact with the world—enabling both TLS interception for semantic \emph{intent} and syscall monitoring for system \emph{actions} with minimal overhead. Critically, its kernel-enforced safety guarantees, including verified termination and memory safety, make it ideal for production environments and provide a stable foundation for our solution~\cite{kerneldoc}.


% \subsection{Observability for LLM Agent}


% Observability has historically been critical for building trustworthy and maintainable software systems. Early research, such as Shortliffe et al. pioneering work on the MYCIN expert system in 1975, highlighted the importance of transparent reasoning, establishing foundational concepts for interpreting and monitoring agent-based systems~\cite{Shortliffe1975Mycin}. With the advent of modern large language model (LLM)-driven systems, the complexity and autonomy of AI agents have dramatically increased, amplifying the necessity for enhanced observability. Contemporary AI agents frequently exhibit emergent and non-deterministic behaviors, making traditional observability approaches insufficient for ensuring system reliability, safety, and correctness~\cite{Dong2024AgentOps,Noble2025IBM}.

% Recent work in the domain, such as the \emph{AgentOps} taxonomy developed by Dong et al., underscores the necessity of capturing detailed artifacts such as prompts, tool invocations, and memory interactions as first-class telemetry data for comprehensive observability~\cite{Dong2024AgentOps}. Moshkovich and Zeltyn expand upon this perspective by proposing a structured six-stage automation pipeline that leverages observational data to enable automated analysis and mitigation strategies, thus addressing critical operational challenges when observability data is incomplete or unavailable in production scenarios~\cite{Moshkovich2025Pipeline}.

% Furthermore, frameworks such as \emph{Watson} proposed by Rombaut et al. delve deeper into cognitive-level observability by reconstructing an LLM's implicit reasoning traces, thereby making the agent's internal cognitive processes explicitly visible~\cite{Rombaut2025Watson}. Complementing this, Kim et al. advocate for \emph{agentic interpretability}, wherein LLMs engage in interactive dialogues, actively explaining their reasoning and behaviors to users, effectively positioning the model itself as a cooperative participant in achieving greater transparency and observability~\cite{Kim2025AgenticInterp}.

% From an industry perspective, emerging standards such as those developed by the OpenTelemetry GenAI working group seek to unify observability practices by establishing common semantic conventions for tracing and logging agent actions across diverse frameworks~\cite{Liu2025OTel,Bandurchin2025Uptrace}. Additionally, tools like Langfuse and Datadog provide practical platforms for capturing detailed trace information and visualizing multi-agent workflows, enhancing developers' ability to diagnose and address complex agent interactions effectively~\cite{Maierhofer2025Langfuse,Datadog2023Agents}.

% In sum, the evolving landscape of AI agent observability highlights the critical need for advanced, robust, and integrated observability solutions that accommodate the unique challenges posed by autonomous and dynamically evolving agent behaviors.

% Despite its recognized importance, existing approaches to observability are insufficient due to the special characteristics of AI agents. The autonomy of AI agents creates a fundamental challenge that invalidates traditional monitoring paradigms: the \textbf{semantic gap}. This gap is the chasm between an agent's high-level, semantic \emph{intent} (the "why," captured in LLM interactions) and its low-level system \emph{actions} (the "what," observed as system calls). This chasm is created by two core agent behaviors: \textbf{Dynamic Execution Paths}, where an agent's operational sequence emerges from non-deterministic reasoning, and \textbf{Cross-Boundary Interactions}, where agents spawn subprocesses (\texttt{bash}, \texttt{curl}, \texttt{git}) that escape the monitoring scope of the parent process. 

% The danger of the unbridged semantic gap is not only theoretical. It manifests in significant reliability and security risks, as demonstrated by these real-world failure scenarios. In an \textbf{Unintended System Modification} incident, an AI agent tasked with code review misinterprets its instructions and begins modifying the production codebase—the semantic gap is exposed when application-level tools see the benign \emph{intent} ("review code") but are completely blind to the destructive system \emph{actions} (\texttt{git commit} and file modifications) occurring in a subprocess. A \textbf{Costly Reasoning Loop} occurs when a data analysis agent enters an infinite cycle calling expensive LLM APIs to solve an impossible problem, consuming thousands of dollars—framework logs reveal the \emph{action} (high volume of API calls) but provide no insight into the semantic \emph{intent}, which is a looping pattern of thought, making it impossible to distinguish between productive work and a costly failure mode without bridging the gap. In a \textbf{Cross-Process Exploitation}, an agent compromised via prompt injection writes a malicious shell script to \texttt{/tmp} and executes it—the semantic gap here is critical as system monitors see the file-write and process-execution \emph{actions}, but without linking them to the malicious \emph{intent} from the prompt, they appear as benign, uncorrelated events, and only by bridging the gap can the full attack narrative be reconstructed. These incidents exemplify a new threat model unique to AI agents, where the disconnect between intent and action can lead to prompt injection attacks, goal drift, and unintended capability escalation.

% Because of this gap, observing only the intent or only the action is insufficient and misleading, demanding a new approach that can holistically link the two.

% \textbf{SDK-Based and Proxy-Based Instrumentation} solutions like LangSmith~\cite{langsmith}, Langfuse~\cite{langfuse}, and Helicone~\cite{helicone} operate on the \textbf{intent side} of the gap. They excel at capturing LLM prompts, responses, and tool choices. However, they are blind to any system \textbf{actions} that occur outside the instrumented framework, such as operations within a spawned shell script. Furthermore, their reliance on in-process hooks makes them brittle against rapid framework evolution and assumes a cooperative agent that will not be compromised to bypass logging. Noble et al.~\cite{Noble2025IBM} further illustrate these limitations, noting SDK-based observability's inability to reliably capture real-time malicious intent or system-level anomalies.

% \textbf{Generic System-Level Monitoring} tools like Falco~\cite{falco} and Tracee~\cite{tracee} operate on the \textbf{action side}. They provide comprehensive visibility into every system call and process execution but lack all semantic context. To these tools, an agent writing a file is just a file write; they cannot determine the \textbf{intent} behind it. As demonstrated in the exploitation scenario, they can report that a script was executed from \texttt{/tmp}, but cannot link this action back to the agent's reasoning, failing to distinguish a malicious attack from a legitimate task.

% Furthermore, reasoning-level observability remains underexplored. Rombaut et al.~\cite{Rombaut2025Watson} highlight the critical value of cognitive observability, where agent reasoning traces and implicit chains-of-thought are reconstructed for debugging, an aspect typically missed by conventional monitoring.

% No existing solution can simultaneously capture semantic intent and system actions, maintain visibility across process boundaries, and remain resilient to framework changes. This critical failure to bridge the semantic gap motivates our novel approach.