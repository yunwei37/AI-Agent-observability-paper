
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
  \country{China}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
  \country{USA}
}


\sloppy
\begin{abstract}
AI agent systems are transforming software infrastructure through autonomous task orchestration, but their production deployment reveals a fundamental mismatch: traditional observability assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving internal logic. Current solutions—traditional APM, model-centric LLM monitoring, and framework-specific instrumentation—create fragmented SDK silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. We propose a two-plane architecture: (1) a Data Plane providing zero-instrumentation telemetry capture at stable system boundaries (kernel, network, TLS), decoupling observability from agent internals, and (2) a Cognitive Plane where autonomous observability agents provide semantic understanding and proactive management that human operators cannot achieve at scale. This approach addresses the unique demands of agent systems while enabling secure, cost-effective, and semantically-aware observability.
\end{abstract}


\maketitle



\section{Introduction: The Agent Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows: debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state—all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{1. Semantic Failures Replace Deterministic Errors.} In our code review example, the agent might hallucinate a security vulnerability that doesn't exist, enter an infinite loop requesting more context, or forget critical style guidelines mid-review. Unlike traditional crashes (segfaults, exceptions), these failures are \emph{semantic}—plausible but incorrect outputs that require understanding agent \emph{intent} to detect. More critically, prompt injection attacks~\cite{indirect-prompt-inject} can compromise agents to evade their own logging, hiding malicious behavior. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{2. Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer: token usage for LLM calls, vector database queries, API calls to GitHub, subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{3. Fragmented Multi-Vendor Instrumentation.} Our code review agent's execution spans multiple administrative domains: LLM serving (OpenAI API), agent orchestration (LangChain), vector storage (Pinecone), tool execution (subprocess calls), and inter-agent communication (custom APIs). Each layer has its own SDK, logging format, and instrumentation requirements, creating incompatible telemetry silos. When the security agent coordination fails, debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{Traditional vs. Agent Observability: A Comparative Framework}
  \label{tab:diff}
  \centering
  \begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
    \toprule
    \textbf{Aspect} &
    \textbf{Traditional Observability} &
    \textbf{AI Agent Observability} \\
    \midrule
    Primary Goal &
    System health \& performance &
    Behavioral correctness, safety, \& trust \\
    Core Pillars &
    Metrics, Events, Logs, Traces (MELT) &
    MELT + \textbf{Evaluations} + \textbf{Governance} \\
    Nature of Failures &
    Crashes, exceptions, latency spikes &
    ``Quiet failures'' (hallucinations, flawed logic, misuse of tools) \\
    System Behavior &
    Deterministic \& predictable &
    Non-deterministic \& emergent \\
    Key Question &
    ``Is the system working?'' &
    ``Is the system thinking correctly and acting appropriately?'' \\
    Core Unit of Analysis &
    Service/request trace &
    Agent decision path/trajectory graph \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics.} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures—crashes, 5xx errors, memory leaks, latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality—capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary.} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior.} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance—infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable.} Returning to our code review agent: suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior—for example, writing a bash script with exploit commands (not logged as harmful file I/O) and then executing it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent.} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability lacks semantic primitives: attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed LLM/AI agent observability tooling as of early 2025, focusing on production-ready solutions that (a) provide SDKs, proxies, or specifications for integration with agent frameworks, and (b) offer telemetry capture, evaluation, or monitoring capabilities for LLM-based systems. Table 2 summarizes 12 representative industrial and open-source solutions, categorized by integration approach, capabilities, and licensing.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: The Limits of Current Approaches}

Analysis of the surveyed tools reveals four critical limitations:

\textbf{(1) Universal SDK Dependence:} All 12 tools require application-level instrumentation, creating maintenance burden and tampering vulnerabilities. \textbf{(2) Immature Semantic Conventions:} Despite OpenTelemetry adoption by five tools~\cite{Liu2025OTel}, agent-specific attributes remain unstandardized. \textbf{(3) Shallow Semantic Analysis:} Only four tools provide LLM-powered evaluation; none explain \emph{why} decisions were made. \textbf{(4) No System-Level Visibility:} No tool observes kernel syscalls, TLS payloads, or subprocess execution directly.

These findings reflect a deeper problem: existing tools inherit their design from two incompatible observability paradigms, neither suited for production agent systems.

\subsection{Redefining Agent Observability for Production Systems}

Current observability approaches define their scope narrowly, missing critical production requirements:

\textbf{Academic Definition: Agent-Centric Internal Consistency.} Research on agent observability focuses on analyzing individual agents' internal states—tracking whether beliefs, intentions, and actions align, detecting when agents deviate from their specified goals. This psychological model addresses cognitive coherence but ignores system-level concerns: What is the cost across LLM + tools + infrastructure? How do multiple agents coordinate? Can compromised agents evade monitoring?

\textbf{Industrial Definition: Model-Centric Input/Output Analysis.} Existing tools define observability as "actionable insights into agent internals through analyzing each component's inputs/outputs." This model-centric view captures inference-level telemetry (prompts, tokens, latencies) but stops at the LLM boundary. It cannot observe tool execution (subprocesses, system calls), inter-agent communication, or cross-layer cost attribution. When our code review agent spawns \texttt{pylint}, model-centric tools see the API call but miss the actual subprocess execution, environment variables, and network connections.

\textbf{Our Definition: System-Level, Multi-Agent Observability.} Production agent deployment requires observability across the entire execution stack, addressing four critical properties that existing definitions ignore:

\emph{Cost transparency}: Unified attribution across LLM inference (token usage), agent orchestration (API calls), tool execution (subprocess resources), and infrastructure (vector databases, network). When our code review agent enters a reasoning loop, we must track costs accumulating simultaneously at all layers—not just token counts.

\emph{Security with tamper resistance}: Audit trails that compromised agents cannot falsify. When prompt injection causes an agent to write then execute a malicious script, observability must capture both actions immutably at the system boundary, even if the agent silences its own SDK logging.

\emph{Multi-agent coordination}: Visibility into cross-agent communication, delegation, conflict, and emergent behaviors. When our code review agent coordinates with the security agent, we need causal traces showing: Which agent initiated? What information was exchanged? Did coordination succeed or deadlock?

\emph{Cross-layer causality}: Correlating agent intent (goal, reasoning) with LLM calls (prompts, parameters) with system execution (syscalls, network) into unified causal graphs. Understanding \emph{why} the agent spawned \texttt{pylint} requires linking: user intent → agent goal → reasoning step → tool selection → subprocess execution → network traffic.

This redefinition reveals why current solutions are fundamentally inadequate: they optimize for single-agent, single-layer, model-centric monitoring. Production systems require multi-agent, multi-layer, system-centric observability. Achieving this demands an architectural paradigm shift.

\section{A Two-Plane Architecture for Agent Observability}

Production agent deployment exhibits three characteristics that make traditional observability infeasible:

\textbf{Heterogeneity.} Agent execution spans multiple administrative domains—LLM SaaS providers (OpenAI, Anthropic), agent frameworks (LangChain, AutoGen), execution runtimes (containers, VMs), third-party tools (MCP servers, APIs)—each with incompatible instrumentation SDKs. Our code review agent's single workflow crosses five vendor boundaries. Coordinating telemetry across this fragmented landscape through application-level hooks is architecturally untenable.

\textbf{Dynamism.} Agents modify their own logic continuously: prompts evolve through few-shot learning, tools are generated dynamically via code synthesis, workflows change based on runtime feedback. Static instrumentation embedded in agent code becomes obsolete within days. Worse, agents can self-modify to bypass their own logging. Traditional "instrument once, monitor forever" assumptions collapse.

\textbf{Scale.} Production deployments involve thousands of agents generating millions of semantically-rich events per hour—LLM reasoning traces, tool invocations, inter-agent messages, state updates. Human operators cannot parse this volume, especially when failures require understanding natural language intent, multi-step reasoning quality, and cross-layer causal chains. The cognitive gap between raw telemetry and actionable insight exceeds human capacity.

These characteristics create two inescapable requirements:

\textbf{Requirement 1 (from Heterogeneity + Dynamism):} Observability must decouple from application internals, capturing telemetry at stable system boundaries that remain invariant across vendor changes, framework updates, and agent self-modification. This necessitates a \textbf{Data Plane} providing zero-instrumentation capture at kernel, network, and TLS interfaces—a unified foundation independent of agent implementation details.

\textbf{Requirement 2 (from Scale + Semantics):} Understanding agent behavior at production scale requires autonomous intelligence—systems that interpret natural language prompts, correlate multi-layer telemetry, infer causal relationships, and adapt to evolving agent behaviors. Only LLM-powered systems can bridge the semantic gap between raw syscalls and agent intent. This necessitates a \textbf{Cognitive Plane} where specialized observability agents monitor, diagnose, and remediate other agents.

Critically, these planes are interdependent. The Data Plane provides tamper-resistant, unified telemetry that the Cognitive Plane requires for trustworthy analysis. The Cognitive Plane provides semantic understanding that makes Data Plane events actionable. Neither can function effectively alone—they form an integrated architecture where system-boundary capture enables intelligent understanding, and intelligent understanding validates system-boundary events. We now detail each plane's design.

\subsection{The Data Plane: Unified, Zero-Instrumentation Telemetry Capture}

The Data Plane addresses Requirement 1 by capturing telemetry at stable system boundaries that remain invariant despite heterogeneous frameworks, dynamic agent evolution, and multi-vendor fragmentation.

\textbf{Architecture: Three Observability Boundaries.} Agent execution crosses three stable interfaces where the Data Plane can intercept comprehensive telemetry without application instrumentation:

\emph{Network boundary}: TLS-encrypted traffic between agent runtime and external services (LLM APIs, vector databases, tool endpoints). By hooking userspace TLS functions (OpenSSL \texttt{SSL\_read/write}, Go \texttt{crypto/tls})~\cite{zheng2025extending}, the Data Plane captures full JSON payloads—prompts, reasoning traces, model parameters, API responses—without proxies or SDK modifications. When our code review agent queries OpenAI's API, we observe: prompt content, temperature setting, token counts, response latencies.

\emph{System boundary}: Kernel syscalls revealing tool execution, file access, subprocess spawning. eBPF tracing~\cite{brendangregg,ebpfio} monitors \texttt{execve()}, \texttt{open()}, \texttt{connect()}, \texttt{write()} with <3\% overhead. When the agent spawns \texttt{pylint}, we capture: exact command arguments, environment variables, stdout/stderr, exit codes, resource consumption.

\emph{Inter-agent boundary}: Network messages, shared memory, or API calls coordinating multiple agents. Combining network tracing with process genealogy, we reconstruct coordination patterns: which agent initiated communication, what data was exchanged, whether handshakes succeeded.

\textbf{Properties Enabling Production Deployment.} This architecture provides four critical capabilities:

\emph{Framework neutrality}: The same Data Plane observes LangChain, AutoGen, custom Python scripts, or future frameworks—no code changes required. Our code review agent remains observable even if completely rewritten.

\emph{Tamper resistance}: Compromised agents cannot falsify kernel-level syscall traces or TLS socket writes. The malicious bash script attack (write exploit file, execute later) becomes visible as correlated \texttt{write()} + \texttt{execve()} events, even if application logs are silenced.

\emph{Cost transparency}: Unified telemetry enables cross-layer attribution. When a reasoning loop occurs, we track simultaneous costs at all layers: LLM tokens (from TLS payloads) + subprocess CPU (from syscalls) + network bandwidth (from socket traffic) + vector DB queries (from HTTPS calls).

\emph{Multi-vendor unification}: By capturing at system boundaries, the Data Plane eliminates SDK fragmentation. Whether the agent uses OpenAI or Anthropic, Pinecone or Weaviate, subprocess tools or HTTP APIs—all telemetry flows through the same unified capture infrastructure.

\textbf{The Semantic Gap Remains.} However, raw Data Plane telemetry cannot answer production-critical questions: \emph{Why} did the agent spawn \texttt{pylint} at this moment? Is this reasoning pattern anomalous? Should we intervene? System boundaries provide comprehensive event streams but lack semantic understanding of agent intent. This limitation necessitates the Cognitive Plane.

\subsection{The Cognitive Plane: Why Only Agents Can Observe Agents}

The Data Plane provides comprehensive, tamper-resistant telemetry. But raw events—\texttt{execve("pylint")}, TLS payload containing prompt text, 5000 tokens consumed—cannot answer: \emph{Why} this decision? Is this behavior anomalous? How should we respond? Bridging telemetry to actionable insight requires intelligence. We argue that only autonomous, LLM-powered systems can provide this intelligence at production scale, for three fundamental reasons:

\textbf{Reason 1: Semantic Failures Require Semantic Understanding.} Agent failures are not crashes or exceptions—they are semantic incorrectness in natural language outputs and reasoning quality. When our code review agent hallucinates a non-existent security vulnerability, no error metric spikes. The only signal is \emph{semantic plausibility despite factual incorrectness}. Detecting this requires understanding code semantics, security concepts, and reasoning coherence—capabilities that rule-based systems cannot provide because "correctness" is context-dependent and open-ended. Only LLM-powered observability agents can interpret natural language intent and evaluate reasoning quality against domain knowledge.

\textbf{Reason 2: Dynamic Evolution Demands Continuous Learning.} Agent behaviors evolve continuously: prompts change weekly, new tools appear daily, coordination patterns shift as workflows adapt. Any static ruleset defining "normal behavior" becomes obsolete within days—requiring constant human re-tuning that does not scale. Observability agents, by contrast, learn from historical incidents, continuously updating their models of normalcy as production agents evolve. When a new tool pattern emerges (parallel linter execution replacing sequential calls), observability agents adapt their baselines automatically. This co-evolution is essential: \emph{observability must evolve as fast as the agents it monitors}.

\textbf{Reason 3: Multi-Layer Causal Reasoning Exceeds Human Capacity.} Understanding agent failures requires correlating evidence across layers. Consider: A cost spike (symptom) occurs because the agent entered a reasoning loop (agent layer) due to a vector database timeout (infrastructure layer) caused by a network partition (system layer). Reconstructing this causal chain from millions of raw events demands: (1) hypothesis generation ("could this be a DB timeout?"), (2) cross-layer evidence correlation (matching network syscalls to LLM retry patterns), (3) counterfactual reasoning ("would reducing temperature prevent loops?"). These tasks require intelligent inference that humans cannot perform at scale—analyzing thousands of agents, each with complex multi-layer interactions—but are natural for LLM-powered diagnoser agents trained on historical failure patterns.

\textbf{Architecture: Four Specialized Agent Types.} The Cognitive Plane comprises coordinated observability agents~\cite{Rombaut2025Watson,Kim2025AgenticInterp,Dong2024AgentOps}, each addressing distinct requirements:

\emph{Observer agents} monitor Data Plane streams, detecting anomalies via learned patterns: reasoning loops (repeated context requests), cost spikes (token usage surge), security violations (\texttt{execve()} of suspicious commands). \emph{Diagnoser agents} perform root-cause analysis by correlating semantic traces (prompt evolution) with system events (syscalls, network patterns), generating causal explanations: "Agent entered loop because vector DB timeout triggered retry logic"~\cite{Moshkovich2025Pipeline}. \emph{Remediator agents} take autonomous actions within safety boundaries: circuit-breaking runaway agents, adjusting model parameters, scaling infrastructure—escalating high-risk changes to human operators. \emph{Cost optimizer agents} analyze usage patterns across all production agents, identifying optimizations: "Replace sequential tool calls with parallel execution" or "Cache frequent vector queries."

\textbf{Critical Dependency on the Data Plane.} The Cognitive Plane's trustworthiness relies entirely on consuming tamper-resistant Data Plane telemetry. If observability agents analyzed only SDK-instrumented logs, compromised production agents could falsify data, evading detection. Only by consuming kernel-level syscall traces and TLS socket captures—events that production agents cannot manipulate—can the Cognitive Plane provide reliable analysis. Conversely, the Data Plane's raw events become actionable only through Cognitive Plane interpretation. \emph{The two planes form an inseparable architecture}: system-boundary capture provides the trusted foundation; autonomous intelligence transforms data into understanding, insight, and automated response. Future observability infrastructure will be inherently agentic—not merely monitored by humans, but understood and managed by specialized agents designed for that purpose.

\section{Open Research Challenges}

Realizing this vision requires sustained research effort across five critical dimensions:

\textbf{Causal Inference for Agent Behavior.} Current systems correlate events but cannot answer counterfactuals: Why did our code review agent choose \texttt{pylint} over \texttt{flake8}? What would have happened with temperature 0.7 instead of 0.3? Causal observability requires techniques from causal inference applied to agent telemetry—identifying decision points, instrumenting counterfactual branches, and constructing causal graphs from observational data.

\textbf{Benchmarks and Evaluation.} The field lacks standardized evaluation frameworks. We need: (1) datasets of real agent failures with ground-truth root causes, (2) metrics for semantic coverage (what fraction of reasoning intent is captured?), (3) adversarial test suites for tamper resistance, and (4) evaluation protocols for Cognitive Plane effectiveness (detection latency, false positive rates, remediation success).

\textbf{Securing the Observability Infrastructure.} If the Cognitive Plane is compromised, attackers gain visibility into all production agents—telemetry data, prompts, coordination patterns. Critical research questions include: How do we build tamper-proof telemetry channels from Data Plane to Cognitive Plane? Can secure multi-party computation protect sensitive traces while enabling collaborative diagnosis? How do we design Byzantine-fault-tolerant observability agents that function correctly even when some agents are malicious?

\textbf{Semantic Reconstruction from System Events.} While eBPF captures syscalls and TLS payloads, inferring high-level agent \emph{intent} requires bridging a semantic abstraction gap. When we observe \texttt{execve("pylint")} followed by network writes to GitHub API, how do we reconstruct the reasoning: "Agent determined code violated style guide, ran linter to generate specific examples, posted structured feedback"? This demands program synthesis, NLP, and machine learning techniques to map low-level events to high-level goals.

\textbf{Standardization of Semantic Conventions.} OpenTelemetry for agents remains nascent. Community consensus is needed on: span attributes for agent semantics (\texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}), trace topology for multi-agent interactions (how to represent coordination, delegation, conflict), evaluation result schemas (hallucination scores, relevance metrics), and unified cost attribution across LLM, tool, and infrastructure layers.

\section{Conclusion}

Production agent systems require a fundamental shift from application-layer instrumentation to system-boundary observability. We propose a two-plane architecture: a Data Plane providing tamper-resistant, framework-neutral telemetry capture at kernel, network, and TLS interfaces; and a Cognitive Plane deploying autonomous observability agents that provide semantic understanding, continuous learning, and multi-layer causal reasoning. These planes are interdependent—system-boundary capture enables trustworthy analysis; autonomous intelligence makes raw events actionable—addressing the core challenge: \textbf{only autonomous intelligence can effectively observe, understand, and manage autonomous intelligence at production scale}. As agent systems transform software infrastructure, observability infrastructure must itself become agentic.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}

