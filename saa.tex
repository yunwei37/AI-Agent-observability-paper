
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
  \country{China}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
  \country{USA}
}


\sloppy
\begin{abstract}
Production AI agent systems expose a fundamental observability mismatch: traditional monitoring assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving logic. Current solutions—APM tools, LLM-centric monitoring, and framework-specific SDKs—create fragmented silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. This paper presents a vision for unified agentic interfaces through a two-plane architecture: a Data Plane capturing telemetry at stable system boundaries (syscalls, network, inference endpoints, human feedback), decoupling observability from agent internals; and a Cognitive Plane deploying autonomous observability agents for semantic understanding at production scale. We identify open research challenges across system-level capture, semantic reconstruction, and standardization required to realize this vision. (127 words)
\end{abstract}


\maketitle



\section{Introduction: The Agentic Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows: debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state—all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{1. Semantic Failures Replace Deterministic Errors.} In our code review example, the agent might hallucinate a security vulnerability that doesn't exist, enter an infinite loop requesting more context, or forget critical style guidelines mid-review. Unlike traditional crashes (segfaults, exceptions), these failures are \emph{semantic}—plausible but incorrect outputs that require understanding agent \emph{intent} to detect. More critically, prompt injection attacks~\cite{indirect-prompt-inject} can compromise agents to evade their own logging, hiding malicious behavior. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{2. Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer: token usage for LLM calls, vector database queries, API calls to GitHub, subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{3. Fragmented Multi-Vendor Instrumentation.} Our code review agent's execution spans multiple administrative domains: LLM serving (OpenAI API), agent orchestration (LangChain), vector storage (Pinecone), tool execution (subprocess calls), and inter-agent communication (custom APIs). Each layer has its own SDK, logging format, and instrumentation requirements, creating incompatible telemetry silos. When the security agent coordination fails, debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{Traditional vs. Agentic Observability: A Comparative Framework}
  \label{tab:diff}
  \centering
  \begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
    \toprule
    \textbf{Aspect} &
    \textbf{Traditional Observability} &
    \textbf{Agentic Observability} \\
    \midrule
    Primary Goal &
    System health \& performance &
    Behavioral correctness, safety, \& trust \\
    Core Pillars &
    Metrics, Events, Logs, Traces (MELT) &
    MELT + \textbf{Evaluations} + \textbf{Governance} \\
    Nature of Failures &
    Crashes, exceptions, latency spikes &
    ``Quiet failures'' (hallucinations, flawed logic, misuse of tools) \\
    System Behavior &
    Deterministic \& predictable &
    Non-deterministic \& emergent \\
    Key Question &
    ``Is the system working?'' &
    ``Is the system thinking correctly and acting appropriately?'' \\
    Core Unit of Analysis &
    Service/request trace &
    Agent decision path/trajectory graph \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics.} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures—crashes, 5xx errors, memory leaks, latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality—capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary.} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior.} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance—infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable.} Returning to our code review agent: suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior—for example, writing a bash script with exploit commands (not logged as harmful file I/O) and then executing it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent.} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability lacks semantic primitives: attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed agentic observability tooling as of early 2025, examining both industrial deployments and academic research. Our analysis covers: (1) \textbf{Industrial tools}—production-ready solutions providing SDKs, proxies, or specifications for agent framework integration (Table 2), and (2) \textbf{Academic research}—foundational work on agent monitoring, interpretability, and evaluation that informs current tooling design.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith}~\cite{langsmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone}~\cite{helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop}~\cite{traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix}~\cite{phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse}~\cite{langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit}~\cite{whylabs} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer}~\cite{promptlayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI}~\cite{literalai} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces}~\cite{wandb} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI}~\cite{honeycomb} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.}~\cite{otelgenai} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference}~\cite{openinference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: The Limits of Current Approaches}

\textbf{Industrial Tools.} Analysis of 12 production systems (Table 2) reveals four critical limitations: \textbf{(1) Universal SDK Dependence}—all tools require application-level instrumentation, creating maintenance burden and tampering vulnerabilities. \textbf{(2) Immature Semantic Conventions}—despite OpenTelemetry adoption by five tools~\cite{Liu2025OTel}, agent-specific attributes remain unstandardized. \textbf{(3) Shallow Semantic Analysis}—only four tools provide LLM-powered evaluation; none explain \emph{why} decisions were made. \textbf{(4) No System-Level Visibility}—no tool observes kernel syscalls, TLS payloads, or subprocess execution directly.

\textbf{Academic Research.} Three research threads address complementary aspects: \textbf{(1) Agent monitoring frameworks}~\cite{Rombaut2025Watson,Dong2024AgentOps} focus on trajectory logging and workflow visualization but assume instrumented runtimes. \textbf{(2) Mechanistic interpretability}~\cite{Kim2025AgenticInterp} reveals model internal reasoning through attention analysis, providing cognitive transparency unavailable in black-box monitoring. \textbf{(3) Evaluation methodologies}~\cite{Moshkovich2025Pipeline} develop semantic correctness metrics (hallucination detection, goal alignment) but operate on logged outputs rather than system-boundary telemetry.

These findings reflect a deeper problem: existing approaches inherit design assumptions from two incompatible paradigms, neither suited for production agentic systems.

\subsection{Redefining Agent Observability for Production Systems}

Current observability approaches define their scope narrowly, missing critical production requirements:

\textbf{Academic Definition: Agent-Centric Internal Consistency.} Research on agent observability focuses on analyzing individual agents' internal states—tracking whether beliefs, intentions, and actions align, detecting when agents deviate from their specified goals. This psychological model addresses cognitive coherence but ignores system-level concerns: What is the cost across LLM + tools + infrastructure? How do multiple agents coordinate? Can compromised agents evade monitoring?

\textbf{Industrial Definition: Model-Centric Input/Output Analysis.} Existing tools define observability as "actionable insights into agent internals through analyzing each component's inputs/outputs." This model-centric view captures inference-level telemetry (prompts, tokens, latencies) but stops at the LLM boundary. It cannot observe tool execution (subprocesses, system calls), inter-agent communication, or cross-layer cost attribution. When our code review agent spawns \texttt{pylint}, model-centric tools see the API call but miss the actual subprocess execution, environment variables, and network connections.

\textbf{Our Definition: System-Level, Multi-Agent Observability.} Production agent deployment requires observability across the entire execution stack, addressing four critical properties that existing definitions ignore:

\emph{Cost transparency}: Unified attribution across LLM inference (token usage), agent orchestration (API calls), tool execution (subprocess resources), and infrastructure (vector databases, network). When our code review agent enters a reasoning loop, we must track costs accumulating simultaneously at all layers—not just token counts.

\emph{Security with tamper resistance}: Audit trails that compromised agents cannot falsify. When prompt injection causes an agent to write then execute a malicious script, observability must capture both actions immutably at the system boundary, even if the agent silences its own SDK logging.

\emph{Multi-agent coordination}: Visibility into cross-agent communication, delegation, conflict, and emergent behaviors. When our code review agent coordinates with the security agent, we need causal traces showing: Which agent initiated? What information was exchanged? Did coordination succeed or deadlock?

\emph{Cross-layer causality}: Correlating agent intent (goal, reasoning) with LLM calls (prompts, parameters) with system execution (syscalls, network) into unified causal graphs. Understanding \emph{why} the agent spawned \texttt{pylint} requires linking: user intent → agent goal → reasoning step → tool selection → subprocess execution → network traffic.

This redefinition reveals why current solutions are fundamentally inadequate: they optimize for single-agent, single-layer, model-centric monitoring. Production systems require multi-agent, multi-layer, system-centric observability.

\textbf{The Path Forward.} Achieving production-grade agentic observability demands resolving two architectural tensions: (1) \emph{Where to capture telemetry?} Application instrumentation is semantically rich but fragile and tamperable; system boundaries are stable and tamper-resistant but semantically opaque. (2) \emph{Who analyzes telemetry?} Human operators understand intent but cannot scale to millions of events; rule-based systems scale but cannot interpret semantic failures. A complete solution must bridge both gaps simultaneously—capturing at stable interfaces while analyzing with autonomous intelligence. We now present a vision for such an architecture.

\section{A Two-Plane Architecture for Agent Observability}

Production agent deployment exhibits three characteristics that make traditional observability infeasible:

\textbf{Heterogeneity.} Agent execution spans multiple administrative domains—LLM SaaS providers (OpenAI, Anthropic), agent frameworks (LangChain, AutoGen), execution runtimes (containers, VMs), third-party tools (MCP servers, APIs)—each with incompatible instrumentation SDKs. Our code review agent's single workflow crosses five vendor boundaries. Coordinating telemetry across this fragmented landscape through application-level hooks is architecturally untenable.

\textbf{Dynamism.} Agents modify their own logic continuously: prompts evolve through few-shot learning, tools are generated dynamically via code synthesis, workflows change based on runtime feedback. Static instrumentation embedded in agent code becomes obsolete within days. Worse, agents can self-modify to bypass their own logging. Traditional "instrument once, monitor forever" assumptions collapse.

\textbf{Scale.} Production deployments involve thousands of agents generating millions of semantically-rich events per hour—LLM reasoning traces, tool invocations, inter-agent messages, state updates. Human operators cannot parse this volume, especially when failures require understanding natural language intent, multi-step reasoning quality, and cross-layer causal chains. The cognitive gap between raw telemetry and actionable insight exceeds human capacity.

These characteristics create two inescapable requirements:

\textbf{Requirement 1 (from Heterogeneity + Dynamism):} Observability must decouple from application internals, capturing telemetry at stable system boundaries that remain invariant across vendor changes, framework updates, and agent self-modification. This necessitates a \textbf{Data Plane} providing zero-instrumentation capture at kernel, network, and TLS interfaces—a unified foundation independent of agent implementation details.

\textbf{Requirement 2 (from Scale + Semantics):} Understanding agent behavior at production scale requires autonomous intelligence—systems that interpret natural language prompts, correlate multi-layer telemetry, infer causal relationships, and adapt to evolving agent behaviors. Only LLM-powered systems can bridge the semantic gap between raw syscalls and agent intent. This necessitates a \textbf{Cognitive Plane} where specialized observability agents monitor, diagnose, and remediate other agents.

Critically, these planes are interdependent. The Data Plane provides tamper-resistant, unified telemetry that the Cognitive Plane requires for trustworthy analysis. The Cognitive Plane provides semantic understanding that makes Data Plane events actionable. Neither can function effectively alone—they form an integrated architecture where system-boundary capture enables intelligent understanding, and intelligent understanding validates system-boundary events. We now detail each plane's design.

\subsection{The Data Plane: Unified, Zero-Instrumentation Telemetry Capture}

The Data Plane addresses Requirement 1 by capturing telemetry at stable system boundaries that remain invariant despite heterogeneous frameworks, dynamic agent evolution, and multi-vendor fragmentation.

\textbf{Architecture: Four Unified Agentic Interfaces.} The Data Plane captures telemetry at stable system boundaries organized into four hierarchical layers, each providing distinct observability capabilities:

\emph{(1) Model Layer—Inference \& Internal States}: Model serving infrastructure exposes two complementary interfaces. The \emph{inference interface} provides GPU/CPU metrics, driver-level statistics (CUDA calls, memory transfers), and framework hooks (PyTorch profiling, TensorFlow events) capturing token throughput, batching efficiency, and hardware utilization. The \emph{internal interface} reveals cognitive processes through mechanistic interpretability~\cite{Kim2025AgenticInterp}—attention patterns, layer activations, intermediate representations—exposing \emph{why} models generate specific outputs. Together, these interfaces bridge infrastructure monitoring and semantic transparency without instrumenting agent code.

\emph{(2) Network Layer—API Calls \& Encrypted Payloads}: TLS-encrypted traffic between agent runtime and external services (LLM APIs, vector databases, tool endpoints). By hooking userspace TLS functions (OpenSSL \texttt{SSL\_read/write}, Go \texttt{crypto/tls})~\cite{zheng2025extending}, we capture full JSON payloads—prompts, reasoning traces, model parameters, API responses—without proxies or SDK modifications. This interface provides complete visibility into agent-environment interactions while maintaining end-to-end encryption.

\emph{(3) System Layer—Kernel Events \& Resource Usage}: Kernel syscalls revealing tool execution, file access, subprocess spawning, and resource consumption. eBPF tracing~\cite{brendangregg,ebpfio} monitors \texttt{execve()}, \texttt{open()}, \texttt{connect()}, \texttt{write()} with <3\% overhead, capturing: command arguments, environment variables, stdout/stderr, exit codes, network connections. This tamper-resistant interface observes agent actions that SDK instrumentation can miss or be instructed to hide.

\emph{(4) Human Layer—Feedback \& Supervision}: Interactive boundaries where humans provide corrections, ratings, approvals, or interventions. Capturing user inputs alongside agent responses (explanations, confidence scores) provides ground truth for evaluation, alignment monitoring, and understanding when human oversight prevented failures. This interface closes the observability loop by incorporating human judgment into automated analysis.

\textbf{Achieving Unification Across Boundaries.} The key challenge is correlating events from heterogeneous sources—GPU metrics, TLS payloads, syscalls, human feedback—into unified causal traces. The Data Plane achieves this through three mechanisms: (1) \emph{Temporal correlation}: aligning timestamps across boundaries using synchronized clocks and causal ordering, (2) \emph{Contextual tagging}: propagating correlation IDs (request IDs, trace IDs, session tokens) across inference calls, network requests, and subprocess execution, and (3) \emph{Graph reconstruction}: building causal dependency graphs that link inference events → network calls → syscalls → human interactions, enabling root-cause analysis across the entire stack.

This unified view provides \emph{framework neutrality} (observes any agent implementation), \emph{tamper resistance} (kernel/hardware events cannot be falsified), \emph{cost transparency} (attributes expenses across all layers), and \emph{multi-vendor compatibility} (works across OpenAI, Anthropic, local models). However, raw telemetry lacks semantic understanding—\emph{why} decisions were made, whether behavior is anomalous, how to respond. This gap necessitates the Cognitive Plane.

\subsection{The Cognitive Plane: Why Only Agents Can Observe Agents}

The Data Plane provides comprehensive, tamper-resistant telemetry. But raw events—\texttt{execve("pylint")}, TLS payload containing prompt text, 5000 tokens consumed—cannot answer: \emph{Why} this decision? Is this behavior anomalous? How should we respond? Bridging telemetry to actionable insight requires intelligence. We argue that only autonomous, LLM-powered systems can provide this intelligence at production scale, for three fundamental reasons:

\textbf{Reason 1: Semantic Failures Require Semantic Understanding.} Agent failures are not crashes or exceptions—they are semantic incorrectness in natural language outputs and reasoning quality. When our code review agent hallucinates a non-existent security vulnerability, no error metric spikes. The only signal is \emph{semantic plausibility despite factual incorrectness}. Detecting this requires understanding code semantics, security concepts, and reasoning coherence—capabilities that rule-based systems cannot provide because "correctness" is context-dependent and open-ended. Only LLM-powered observability agents can interpret natural language intent and evaluate reasoning quality against domain knowledge.

\textbf{Reason 2: Dynamic Evolution Demands Continuous Learning.} Agent behaviors evolve continuously: prompts change weekly, new tools appear daily, coordination patterns shift as workflows adapt. Any static ruleset defining "normal behavior" becomes obsolete within days—requiring constant human re-tuning that does not scale. Observability agents, by contrast, learn from historical incidents, continuously updating their models of normalcy as production agents evolve. When a new tool pattern emerges (parallel linter execution replacing sequential calls), observability agents adapt their baselines automatically. This co-evolution is essential: \emph{observability must evolve as fast as the agents it monitors}.

\textbf{Reason 3: Multi-Layer Causal Reasoning Exceeds Human Capacity.} Understanding agent failures requires correlating evidence across layers. Consider: A cost spike (symptom) occurs because the agent entered a reasoning loop (agent layer) due to a vector database timeout (infrastructure layer) caused by a network partition (system layer). Reconstructing this causal chain from millions of raw events demands: (1) hypothesis generation ("could this be a DB timeout?"), (2) cross-layer evidence correlation (matching network syscalls to LLM retry patterns), (3) counterfactual reasoning ("would reducing temperature prevent loops?"). These tasks require intelligent inference that humans cannot perform at scale—analyzing thousands of agents, each with complex multi-layer interactions—but are natural for LLM-powered diagnoser agents trained on historical failure patterns.

\textbf{Architecture: Four Specialized Agent Types.} The Cognitive Plane comprises coordinated observability agents~\cite{Rombaut2025Watson,Kim2025AgenticInterp,Dong2024AgentOps}, each addressing distinct requirements:

\emph{Observer agents} monitor Data Plane streams, detecting anomalies via learned patterns: reasoning loops (repeated context requests), cost spikes (token usage surge), security violations (\texttt{execve()} of suspicious commands). \emph{Diagnoser agents} perform root-cause analysis by correlating semantic traces (prompt evolution) with system events (syscalls, network patterns), generating causal explanations: "Agent entered loop because vector DB timeout triggered retry logic"~\cite{Moshkovich2025Pipeline}. \emph{Remediator agents} take autonomous actions within safety boundaries: circuit-breaking runaway agents, adjusting model parameters, scaling infrastructure—escalating high-risk changes to human operators. \emph{Cost optimizer agents} analyze usage patterns across all production agents, identifying optimizations: "Replace sequential tool calls with parallel execution" or "Cache frequent vector queries."

\textbf{Architectural Separation of Concerns.} The two-plane design enforces strict separation across three critical dimensions, making each plane independently necessary:

\emph{(1) Trust boundary separation}: The Data Plane operates at privileged system boundaries (kernel, TLS layer, hardware) inaccessible to production agents, providing tamper-resistant audit trails. The Cognitive Plane operates in userspace, analyzing telemetry without privileged access. This separation ensures that compromised production agents cannot falsify the observability data used to detect their compromise—a fundamental security requirement that a monolithic architecture cannot satisfy.

\emph{(2) Technology stack separation}: The Data Plane requires low-level systems programming (eBPF, kernel modules, TLS hooks) optimized for high-throughput, low-overhead capture. The Cognitive Plane requires LLM orchestration, semantic analysis, and adaptive learning—fundamentally different technical requirements. Mixing these concerns in a single system creates architectural complexity and performance bottlenecks.

\emph{(3) Evolution rate separation}: The Data Plane's interfaces (syscalls, TLS protocols, GPU APIs) evolve slowly, governed by OS/hardware vendors with stability guarantees. The Cognitive Plane's analysis logic must evolve rapidly as production agents change weekly—new prompts, tools, coordination patterns. Decoupling these layers allows the Cognitive Plane to adapt continuously while the Data Plane remains stable, avoiding the instrumentation gap that plagues SDK-based monitoring.

\emph{The two planes form an inseparable architecture}: system-boundary capture provides the trusted foundation; autonomous intelligence transforms data into understanding, insight, and automated response. Neither can function effectively alone. Future observability infrastructure will be inherently agentic—not merely monitored by humans, but understood and managed by specialized agents designed for that purpose.

\section{Open Research Challenges}

Realizing this vision requires sustained research effort organized around the two-plane architecture. We identify critical challenges for each plane and their integration:

\subsection{Data Plane Challenges: Capture at Scale}

\textbf{High-Performance Correlation Across Boundaries.} Unifying telemetry from heterogeneous sources (GPU metrics at millisecond granularity, TLS payloads at packet granularity, syscalls at microsecond granularity, human feedback at minute granularity) requires scalable correlation algorithms. Research questions: How do we propagate correlation IDs across boundaries without modifying agent code? Can probabilistic data structures (Bloom filters, HyperLogLog) enable constant-memory correlation at millions of events per second?

\textbf{Tamper-Proof Telemetry Channels.} Securing the Data Plane → Cognitive Plane communication path against compromised production agents requires cryptographic guarantees. Open problems: How do we design append-only, authenticated telemetry streams using hardware security modules or trusted execution environments? Can blockchain-inspired techniques provide verifiable audit trails without performance overhead?

\textbf{Privacy-Preserving Capture.} TLS payload inspection reveals sensitive data (API keys, user prompts, proprietary algorithms). Research is needed on: differential privacy mechanisms for prompt redaction, homomorphic encryption enabling analysis on encrypted telemetry, and federated learning allowing cross-organization anomaly detection without sharing raw data.

\subsection{Cognitive Plane Challenges: Understanding at Scale}

\textbf{Semantic Reconstruction from System Events.} Bridging the abstraction gap between low-level telemetry (\texttt{execve("pylint")}, TLS payload bytes) and high-level intent ("validate code style before posting review") requires techniques from program synthesis, natural language inference, and causal reasoning. Key questions: Can fine-tuned LLMs learn to map syscall sequences to semantic goals? How do we construct training datasets pairing system traces with ground-truth intentions?

\textbf{Causal Inference for Agent Behavior.} Current systems correlate events but cannot answer counterfactuals: Why did the agent choose this tool over alternatives? What would have happened with different hyperparameters? Causal observability demands: identifying decision points in reasoning traces, instrumenting counterfactual branches in production agents for A/B testing, and constructing causal DAGs from observational telemetry.

\textbf{Adaptive Anomaly Detection.} As production agents evolve continuously, static anomaly baselines become obsolete. Research challenges: How do observability agents update their models of "normal behavior" without catastrophic forgetting? Can meta-learning enable few-shot adaptation to new agent types? How do we detect novel attack patterns (adversarial prompts, data poisoning) never seen during training?

\subsection{Cross-Plane Challenges: Integration \& Standardization}

\textbf{Unified Semantic Conventions.} OpenTelemetry for agentic systems remains nascent. Community consensus is needed on: span attributes for reasoning semantics (\texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}), trace topology for multi-agent coordination (representing delegation, conflict, consensus), evaluation result schemas (hallucination scores, alignment metrics), and cost attribution across model inference, tool execution, and infrastructure layers.

\textbf{Benchmarks and Evaluation.} The field lacks standardized evaluation frameworks. Critical needs: (1) datasets of real agentic failures with ground-truth root causes and counterfactual traces, (2) metrics quantifying semantic coverage (what fraction of reasoning intent is captured at each boundary?), (3) adversarial test suites evaluating tamper resistance against prompt injection and SDK evasion, (4) end-to-end benchmarks measuring detection latency, false positive rates, and remediation success across the full architecture.

\textbf{Securing Observability Infrastructure.} If the Cognitive Plane is compromised, attackers gain visibility into all production agents. Open problems: Byzantine-fault-tolerant consensus among observability agents, secure multi-party computation for collaborative diagnosis across organizations, and differential privacy guarantees for telemetry sharing that preserve detection accuracy while protecting sensitive data.

\section{Conclusion}

Production agentic systems demand a paradigm shift from application-layer instrumentation to unified agentic interfaces. We present a vision for observability built on two inseparable planes: a Data Plane capturing tamper-resistant telemetry at stable system boundaries (model inference, network, kernel, human feedback), and a Cognitive Plane deploying autonomous observability agents for semantic understanding at scale. Together, they address the fundamental challenge that \textbf{only autonomous intelligence can effectively observe, understand, and manage autonomous intelligence in production}. Realizing this vision requires sustained research across system-level capture, semantic reconstruction, and standardization—but the direction is clear: as agents transform software infrastructure, observability infrastructure must itself become agentic.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}

