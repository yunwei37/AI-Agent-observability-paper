
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
  \country{China}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
  \country{USA}
}


\sloppy
\begin{abstract}
Production AI agent systems expose a fundamental observability mismatch: traditional monitoring assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving logic. Current solutions (APM tools, LLM-centric monitoring, and framework-specific SDKs) create fragmented silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. This paper presents a vision for unified agentic interfaces through a two-plane architecture: a Data Plane capturing telemetry at stable system boundaries (syscalls, network, inference endpoints, human feedback), decoupling observability from agent internals; and a Cognitive Plane deploying autonomous observability agents for semantic understanding at production scale. We identify open research challenges across system-level capture, semantic reconstruction, and standardization required to realize this vision. (127 words)
\end{abstract}


\maketitle



\section{Introduction: The Agentic Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows, including debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state, all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{1. Semantic Failures Replace Deterministic Errors.} In our code review example, the agent might hallucinate a security vulnerability that doesn't exist, enter an infinite loop requesting more context, or forget critical style guidelines mid-review. Unlike traditional crashes (segfaults, exceptions), these failures are \emph{semantic}, meaning they are plausible but incorrect outputs that require understanding agent \emph{intent} to detect. As practitioners observe, building multi-agent systems without observability "feels like debugging a black box" where developers are "essentially flying blind" without visibility into decisions and data flows~\cite{petropavlov-medium,masood-medium}. More critically, prompt injection attacks~\cite{indirect-prompt-inject} can compromise agents to evade their own logging, hiding malicious behavior. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{2. Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer, including token usage for LLM calls, vector database queries, API calls to GitHub, and subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{3. Fragmented Multi-Vendor Instrumentation.} Our code review agent's execution spans multiple administrative domains, including LLM serving (OpenAI API), agent orchestration (LangChain), vector storage (Pinecone), tool execution (subprocess calls), and inter-agent communication (custom APIs). Each layer has its own SDK, logging format, and instrumentation requirements, creating incompatible telemetry silos. Multi-agent coordination exacerbates costs: production deployments report up to 15× higher token consumption compared to single-agent workflows~\cite{anthropic-multiagent}, while error propagation across agent handoffs creates cascading failures invisible to per-agent monitoring. When the security agent coordination fails, debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{Traditional vs. Agentic Observability: A Comparative Framework}
  \label{tab:diff}
  \centering
  \begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
    \toprule
    \textbf{Aspect} &
    \textbf{Traditional Observability} &
    \textbf{Agentic Observability} \\
    \midrule
    Primary Goal &
    System health \& performance &
    Behavioral correctness, safety, \& trust \\
    Core Pillars &
    Metrics, Events, Logs, Traces (MELT)~\cite{li2022observability} &
    MELT + \textbf{Evaluations} + \textbf{Governance} \\
    Nature of Failures &
    Crashes, exceptions, latency spikes &
    ``Quiet failures'' (hallucinations, flawed logic, misuse of tools) \\
    System Behavior &
    Deterministic \& predictable &
    Non-deterministic \& emergent \\
    Key Question &
    ``Is the system working?'' &
    ``Is the system thinking correctly and acting appropriately?'' \\
    Core Unit of Analysis &
    Service/request trace &
    Agent decision path/trajectory graph \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics.} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures such as crashes, 5xx errors, memory leaks, and latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality, capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary.} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior.} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance, all infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable.} Returning to our code review agent, suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior. For example, it could write a bash script with exploit commands (not logged as harmful file I/O) and then execute it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent.} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability frameworks~\cite{sigelman2010dapper,majors2017observability} lack semantic primitives such as attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards~\cite{otelgenai,Liu2025OTel}, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed agentic observability tooling as of early 2025, examining both industrial deployments and academic research. Our analysis covers two areas: (1) \textbf{Industrial tools}, which are production-ready solutions providing SDKs, proxies, or specifications for agent framework integration (Table 2), and (2) \textbf{Academic research}, which includes foundational work on agent monitoring, interpretability, and evaluation that informs current tooling design.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith}~\cite{langsmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone}~\cite{helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop}~\cite{traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix}~\cite{phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse}~\cite{langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit}~\cite{whylabs} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer}~\cite{promptlayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI}~\cite{literalai} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces}~\cite{wandb} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI}~\cite{honeycomb} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.}~\cite{otelgenai} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference}~\cite{openinference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
13 & \textbf{AgentOps}~\cite{agentops-tool} (2024) & Proxy injection into LLM calls & Time-travel debugging, multi-framework support (CrewAI, AutoGen) & OSS & Session replay across agent frameworks \\
14 & \textbf{TruLens}~\cite{trulens} (2024) & Wrapper (\texttt{TruLlama}) or SDK & Multi-turn session tracking, custom feedback functions & OSS + hosted & Evaluation-focused with feedback loops \\
15 & \textbf{Phospho}~\cite{phospho} (2024) & Log ingestion API & Clustering/labeling of LLM outputs, anomaly detection & OSS & Post-hoc NLP analytics on collected data \\
16 & \textbf{MLflow}~\cite{mlflow-genai} (2024) & \texttt{mlflow.autolog()} for LLMs & Experiment tracking, artifact logging (prompts/outputs) & Apache-2.0 & General MLOps extended to generative AI \\
17 & \textbf{Maxim AI}~\cite{maxim-ai} (2024) & One-line SDK integration & Agent trajectory visualization, cost/latency analytics & SaaS & Polished dashboard for production monitoring \\
18 & \textbf{Guardrails.AI}~\cite{guardrails-ai} (2023) & Input/output validators & Real-time safety checks (toxicity, PII), auto-retry on violations & OSS (Apache-2.0) & Observability through safety enforcement \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: The Limits of Current Approaches}

\textbf{Industrial Tools.} Analysis of 18 production systems (Table 2) reveals four critical limitations: \textbf{(1) Universal SDK Dependence}: all tools require application-level instrumentation, creating maintenance burden and tampering vulnerabilities. \textbf{(2) Immature Semantic Conventions}: despite OpenTelemetry adoption by five tools~\cite{Liu2025OTel}, agent-specific attributes remain unstandardized. \textbf{(3) Shallow Semantic Analysis}: while tools like TruLens and Arize Phoenix provide LLM-powered evaluation, none explain \emph{why} decisions were made through reasoning trace reconstruction. \textbf{(4) No System-Level Visibility}: no tool observes kernel syscalls, TLS payloads, or subprocess execution directly.

\textbf{Academic Research.} Four research threads address complementary aspects: \textbf{(1) Agent monitoring frameworks}~\cite{Rombaut2025Watson,Dong2024AgentOps} focus on trajectory logging and workflow visualization but assume instrumented runtimes. \textbf{(2) Mechanistic interpretability}~\cite{Kim2025AgenticInterp} reveals model internal reasoning through attention analysis, providing cognitive transparency unavailable in black-box monitoring. \textbf{(3) Evaluation methodologies}~\cite{Moshkovich2025Pipeline} develop semantic correctness metrics (hallucination detection, goal alignment) but operate on logged outputs rather than system-boundary telemetry. \textbf{(4) System-level observability}~\cite{zheng2025agentsight} using eBPF to capture kernel events and TLS payloads, correlating system-level execution with semantic intent; however, this approach only observes post-hoc—capturing what already happened without providing mechanisms for real-time intervention, lacks standardized semantic schemas for multi-agent coordination, and cannot observe model-internal reasoning (attention patterns, layer activations) that explain \emph{why} specific outputs were generated

These findings reflect a deeper problem: existing approaches inherit design assumptions from two incompatible paradigms, neither suited for production agentic systems.

\subsection{Redefining Agent Observability for Production Systems}

Current observability approaches define their scope narrowly, missing critical production requirements:

\textbf{Academic Definition: Agent-Centric Internal Consistency.} Research on agent observability focuses on analyzing individual agents' internal states, tracking whether beliefs, intentions, and actions align, and detecting when agents deviate from their specified goals. This psychological model addresses cognitive coherence but ignores system-level concerns such as the cost across LLM + tools + infrastructure, how multiple agents coordinate, and whether compromised agents can evade monitoring.

\textbf{Industrial Definition: Model-Centric Input/Output Analysis.} Existing tools define observability as "actionable insights into agent internals through analyzing each component's inputs/outputs." This model-centric view captures inference-level telemetry (prompts, tokens, latencies) but stops at the LLM boundary. It cannot observe tool execution (subprocesses, system calls), inter-agent communication, or cross-layer cost attribution. When our code review agent spawns \texttt{pylint}, model-centric tools see the API call but miss the actual subprocess execution, environment variables, and network connections.

\textbf{Our Definition: System-Level, Multi-Agent Observability.} Production agent deployment requires observability across the entire execution stack, addressing four critical properties that existing definitions ignore:

\emph{Cost transparency}: Unified attribution across LLM inference (token usage), agent orchestration (API calls), tool execution (subprocess resources), and infrastructure (vector databases, network). When our code review agent enters a reasoning loop, we must track costs accumulating simultaneously at all layers, not just token counts.

\emph{Security with tamper resistance}: Audit trails that compromised agents cannot falsify. When prompt injection causes an agent to write then execute a malicious script, observability must capture both actions immutably at the system boundary, even if the agent silences its own SDK logging.

\emph{Multi-agent coordination}: Visibility into cross-agent communication, delegation, conflict, and emergent behaviors. When our code review agent coordinates with the security agent, we need causal traces showing which agent initiated, what information was exchanged, and whether coordination succeeded or deadlocked.

\emph{Cross-layer causality}: Correlating agent intent (goal, reasoning) with LLM calls (prompts, parameters) with system execution (syscalls, network) into unified causal graphs. Understanding \emph{why} the agent spawned \texttt{pylint} requires linking user intent → agent goal → reasoning step → tool selection → subprocess execution → network traffic.

This redefinition reveals why current solutions are fundamentally inadequate: they optimize for single-agent, single-layer, model-centric monitoring. Production systems require multi-agent, multi-layer, system-centric observability.

\textbf{The Path Forward.} Achieving production-grade agentic observability demands resolving two architectural tensions: (1) \emph{Where to capture telemetry?} Application instrumentation is semantically rich but fragile and tamperable; system boundaries are stable and tamper-resistant but semantically opaque. (2) \emph{Who analyzes telemetry?} Human operators understand intent but cannot scale to millions of events; rule-based systems scale but cannot interpret semantic failures. A complete solution must bridge both gaps simultaneously: capturing at stable interfaces while analyzing with autonomous intelligence. We now present a vision for such an architecture.

\section{A Two-Plane Architecture for Agent Observability}

Production agent deployment exhibits three characteristics that make traditional observability infeasible:

\textbf{Heterogeneity.} Agent execution spans multiple administrative domains including LLM SaaS providers (OpenAI, Anthropic), agent frameworks (LangChain, AutoGen), execution runtimes (containers, VMs), and third-party tools (MCP servers, APIs), each with incompatible instrumentation SDKs. Our code review agent's single workflow crosses five vendor boundaries. Coordinating telemetry across this fragmented landscape through application-level hooks is architecturally untenable.

\textbf{Dynamism.} Agents modify their own logic continuously. Prompts evolve through few-shot learning, tools are generated dynamically via code synthesis, and workflows change based on runtime feedback. Static instrumentation embedded in agent code becomes obsolete within days. Worse, agents can self-modify to bypass their own logging. Traditional "instrument once, monitor forever" assumptions collapse.

\textbf{Scale.} Production deployments involve thousands of agents generating millions of semantically-rich events per hour, including LLM reasoning traces, tool invocations, inter-agent messages, and state updates. Human operators cannot parse this volume, especially when failures require understanding natural language intent, multi-step reasoning quality, and cross-layer causal chains. The cognitive gap between raw telemetry and actionable insight exceeds human capacity.

These characteristics create two inescapable requirements:

\textbf{Requirement 1 (from Heterogeneity + Dynamism):} Observability must decouple from application internals, capturing telemetry at stable system boundaries that remain invariant across vendor changes, framework updates, and agent self-modification. This necessitates a \textbf{Data Plane} providing zero-instrumentation capture at kernel, network, and TLS interfaces, creating a unified foundation independent of agent implementation details.

\textbf{Requirement 2 (from Scale + Semantics):} Understanding agent behavior at production scale requires autonomous intelligence, including systems that interpret natural language prompts, correlate multi-layer telemetry, infer causal relationships, and adapt to evolving agent behaviors. Only LLM-powered systems can bridge the semantic gap between raw syscalls and agent intent. This necessitates a \textbf{Cognitive Plane} where specialized observability agents monitor, diagnose, and remediate other agents.

Critically, these planes are interdependent. The Data Plane provides tamper-resistant, unified telemetry that the Cognitive Plane requires for trustworthy analysis. The Cognitive Plane provides semantic understanding that makes Data Plane events actionable. Neither can function effectively alone. They form an integrated architecture where system-boundary capture enables intelligent understanding, and intelligent understanding validates system-boundary events. We now detail each plane's design.

\subsection{The Data Plane: Unified, Zero-Instrumentation Telemetry Capture}

The Data Plane addresses Requirement 1 by capturing telemetry at stable system boundaries that remain invariant despite heterogeneous frameworks, dynamic agent evolution, and multi-vendor fragmentation.

\textbf{Architecture: Four Unified Agentic Interfaces.} Building on the AgentOps taxonomy~\cite{Dong2024AgentOps} that identifies key artifacts requiring observability (goals, plans, tool outputs), we extend this to system-level boundaries. To achieve production-grade observability across heterogeneous agent ecosystems, the Data Plane's telemetry capture interfaces must satisfy six essential properties organized into three categories. First, \emph{Deployment independence} requires (1) \emph{stable} capture points anchored at OS/hardware boundaries (syscalls, TLS protocols, GPU APIs) that evolve slowly under vendor guarantees rather than at application code that changes continuously, and (2) \emph{zero-instrumentation} that eliminates SDK imports and code modifications, making observability infrastructure independent of agent implementation.\footnote{While frameworks like LlamaIndex offer "one-click" SDK integration (e.g., \texttt{set\_global\_handler("arize\_phoenix")}), these still require modifying agent codebases and remain vulnerable to tampering. The Data Plane's zero-instrumentation approach eliminates this dependency entirely.} Second, \emph{Security and reliability} demands (3) \emph{tamper-resistant} capture operating at kernel/hardware level where compromised agents cannot falsify telemetry, and (4) \emph{programmable} interfaces providing extensible logic via safe in-kernel execution (eBPF) and userspace hooks that enable custom filters and correlation without system restarts. Third, \emph{Production efficiency} needs (5) \emph{minimal overhead} maintaining sub-3\% performance impact through zero-copy data paths and in-kernel filtering, and (6) \emph{semantically rich} capture going beyond traditional metrics to include natural language prompts, reasoning traces, tool arguments, and causal relationships. Guided by these principles, the Data Plane organizes telemetry capture into four hierarchical layers:

\emph{(1) Model Layer: Inference \& Internal States}: Model serving infrastructure exposes two complementary interfaces. The \emph{inference interface} provides GPU/CPU metrics, driver-level statistics (CUDA calls, memory transfers), and framework hooks (PyTorch profiling, TensorFlow events) capturing token throughput, batching efficiency, and hardware utilization. The \emph{internal interface} reveals cognitive processes through mechanistic interpretability~\cite{Kim2025AgenticInterp}, including attention patterns, layer activations, and intermediate representations, exposing \emph{why} models generate specific outputs. Together, these interfaces bridge infrastructure monitoring and semantic transparency without instrumenting agent code.

\emph{(2) Network Layer: API Calls \& Encrypted Payloads}: TLS-encrypted traffic between agent runtime and external services (LLM APIs, vector databases, tool endpoints). By hooking userspace TLS functions (OpenSSL \texttt{SSL\_read/write}, Go \texttt{crypto/tls})~\cite{zheng2025extending}, we capture full JSON payloads including prompts, reasoning traces, model parameters, and API responses without proxies or SDK modifications. This programmable interface allows custom filtering of sensitive data while maintaining complete visibility into agent-environment interactions.

\emph{(3) System Layer: Kernel Events \& Resource Usage}: Kernel syscalls revealing tool execution, file access, subprocess spawning, and resource consumption. eBPF tracing~\cite{brendangregg,ebpfio} monitors \texttt{execve()}, \texttt{open()}, \texttt{connect()}, \texttt{write()} with <3\% overhead, capturing command arguments, environment variables, stdout/stderr, exit codes, and network connections. This tamper-resistant, programmable interface (via eBPF's safe in-kernel VM) observes agent actions that SDK instrumentation can miss or be instructed to hide.

\emph{(4) Human Layer: Feedback \& Supervision}: Interactive boundaries where humans provide corrections, ratings, approvals, or interventions. Capturing user inputs alongside agent responses (explanations, confidence scores) provides ground truth for evaluation, alignment monitoring, and understanding when human oversight prevented failures. This interface closes the observability loop by incorporating human judgment into automated analysis.

\textbf{Achieving Unification Across Boundaries.} The key challenge is correlating events from heterogeneous sources (GPU metrics, TLS payloads, syscalls, human feedback) into unified causal traces. While prior work like AgentSight~\cite{zheng2025agentsight} demonstrates boundary tracing via eBPF, pure system-level observability cannot observe model internals (attention patterns, GPU memory), provide real-time intervention (only post-hoc forensics), or reliably attribute events in multi-agent scenarios without application context. The Data Plane addresses these gaps through architectural unification across four layers (Model, Network, System, Human) rather than a single boundary, plus three complementary correlation mechanisms: (1) \emph{Temporal correlation} aligning timestamps across boundaries using synchronized clocks and causal ordering, (2) \emph{Contextual tagging} propagating correlation IDs (request IDs, trace IDs, session tokens) across inference calls, network requests, and subprocess execution—enabling attribution in multi-agent environments, and (3) \emph{Graph reconstruction} building causal dependency graphs that link inference events → network calls → syscalls → human interactions, enabling root-cause analysis across the entire stack. By capturing at \emph{four complementary boundaries} rather than a single system layer, the Data Plane bridges the gap between model-internal reasoning (attention patterns), semantic intent (prompts), system execution (syscalls), and human oversight (feedback).

This unified architecture delivers critical production guarantees including \emph{framework neutrality} (observes any agent implementation), \emph{tamper resistance} (kernel/hardware events cannot be falsified), \emph{cost transparency} (attributes expenses across all layers), \emph{multi-vendor compatibility} (works across OpenAI, Anthropic, local models), and \emph{production-grade performance} (minimal overhead via in-kernel filtering and zero-copy data paths). However, raw telemetry lacks semantic understanding about \emph{why} decisions were made, whether behavior is anomalous, and how to respond. This gap necessitates the Cognitive Plane.

\subsection{The Cognitive Plane: Why Only Agents Can Observe Agents}

The Data Plane provides comprehensive, tamper-resistant telemetry. But raw events such as \texttt{execve("pylint")}, TLS payload containing prompt text, or 5000 tokens consumed cannot answer critical questions: \emph{Why} this decision? Is this behavior anomalous? How should we respond? Bridging telemetry to actionable insight requires intelligence. Building on Watson's concept of cognitive observability~\cite{Rombaut2025Watson} and recent work on agentic interpretability~\cite{Kim2025AgenticInterp}, we argue that only autonomous, LLM-powered systems can provide this intelligence at production scale, for three fundamental reasons:

\textbf{Reason 1: Semantic Failures Require Semantic Understanding.} Agent failures are not crashes or exceptions; they are semantic incorrectness in natural language outputs and reasoning quality. When our code review agent hallucinates a non-existent security vulnerability, no error metric spikes. The only signal is \emph{semantic plausibility despite factual incorrectness}. Detecting this requires understanding code semantics, security concepts, and reasoning coherence, capabilities that rule-based systems cannot provide because "correctness" is context-dependent and open-ended. Watson's cognitive observability framework~\cite{Rombaut2025Watson} demonstrates this principle: by retroactively inferring implicit reasoning traces from agent behavior through prompt attribution, observability agents can reconstruct \emph{why} agents made specific decisions, exposing latent chain-of-thought steps unavailable through traditional logging. Only LLM-powered observability agents can interpret natural language intent and evaluate reasoning quality against domain knowledge.

\textbf{Reason 2: Dynamic Evolution Demands Continuous Learning.} Agent behaviors evolve continuously. Prompts change weekly, new tools appear daily, and coordination patterns shift as workflows adapt. Any static ruleset defining "normal behavior" becomes obsolete within days, requiring constant human re-tuning that does not scale. Observability agents, by contrast, learn from historical incidents, continuously updating their models of normalcy as production agents evolve. When a new tool pattern emerges (parallel linter execution replacing sequential calls), observability agents adapt their baselines automatically. This co-evolution is essential: \emph{observability must evolve as fast as the agents it monitors}.

\textbf{Reason 3: Multi-Layer Causal Reasoning Exceeds Human Capacity.} Understanding agent failures requires correlating evidence across layers. Consider this example: A cost spike (symptom) occurs because the agent entered a reasoning loop (agent layer) due to a vector database timeout (infrastructure layer) caused by a network partition (system layer). Reconstructing this causal chain from millions of raw events demands (1) hypothesis generation ("could this be a DB timeout?"), (2) cross-layer evidence correlation (matching network syscalls to LLM retry patterns), and (3) counterfactual reasoning ("would reducing temperature prevent loops?"). These tasks require intelligent inference that humans cannot perform at scale when analyzing thousands of agents, each with complex multi-layer interactions, but are natural for LLM-powered diagnoser agents trained on historical failure patterns.

\textbf{Architecture: Four Specialized Agent Types.} The Cognitive Plane comprises coordinated observability agents~\cite{Rombaut2025Watson,Kim2025AgenticInterp,Dong2024AgentOps}, each addressing distinct requirements:

\emph{Observer agents} monitor Data Plane streams, detecting anomalies via learned patterns such as reasoning loops (repeated context requests), cost spikes (token usage surge), and security violations (\texttt{execve()} of suspicious commands). These agents leverage LLM-as-judge evaluation, where one LLM grades another agent's trajectory against semantic rubrics (e.g., "Did the agent use the correct tool sequence?" or "Is the reasoning coherent?"), automating detection of inefficient loops, missing steps, or hallucinated outputs without human labeling. \emph{Diagnoser agents} perform root-cause analysis by correlating semantic traces (prompt evolution) with system events (syscalls, network patterns), generating causal explanations such as "Agent entered loop because vector DB timeout triggered retry logic"~\cite{Moshkovich2025Pipeline}. \emph{Remediator agents} take autonomous actions within safety boundaries, including circuit-breaking runaway agents, adjusting model parameters, and scaling infrastructure, while escalating high-risk changes to human operators. \emph{Cost optimizer agents} analyze usage patterns across all production agents, identifying optimizations such as "Replace sequential tool calls with parallel execution" or "Cache frequent vector queries."

\textbf{Architectural Separation of Concerns.} The two-plane design enforces strict separation across three critical dimensions, making each plane independently necessary:

\emph{(1) Trust boundary separation}: The Data Plane operates at privileged system boundaries (kernel, TLS layer, hardware) inaccessible to production agents, providing tamper-resistant audit trails. The Cognitive Plane operates in userspace, analyzing telemetry without privileged access. This separation ensures that compromised production agents cannot falsify the observability data used to detect their compromise, a fundamental security requirement that a monolithic architecture cannot satisfy.

\emph{(2) Technology stack separation}: The Data Plane requires low-level systems programming (eBPF, kernel modules, TLS hooks) optimized for high-throughput, low-overhead capture. The Cognitive Plane requires LLM orchestration, semantic analysis, and adaptive learning, which are fundamentally different technical requirements. Mixing these concerns in a single system creates architectural complexity and performance bottlenecks.

\emph{(3) Evolution rate separation}: The Data Plane's interfaces (syscalls, TLS protocols, GPU APIs) evolve slowly, governed by OS/hardware vendors with stability guarantees. The Cognitive Plane's analysis logic must evolve rapidly as production agents change weekly with new prompts, tools, and coordination patterns. Decoupling these layers allows the Cognitive Plane to adapt continuously while the Data Plane remains stable, avoiding the instrumentation gap that plagues SDK-based monitoring.

\emph{The two planes form an inseparable architecture}. System-boundary capture provides the trusted foundation, while autonomous intelligence transforms data into understanding, insight, and automated response. Neither can function effectively alone. Future observability infrastructure will be inherently agentic, not merely monitored by humans, but understood and managed by specialized agents designed for that purpose.

\section{Open Research Challenges}

Realizing this vision requires sustained research effort organized around the two-plane architecture. We identify critical challenges for each plane and their integration:

\subsection{Data Plane Challenges: Capture at Scale}

\textbf{High-Performance Correlation Across Boundaries.} Unifying telemetry from heterogeneous sources (GPU metrics at millisecond granularity, TLS payloads at packet granularity, syscalls at microsecond granularity, human feedback at minute granularity) requires scalable correlation algorithms. Research questions: How do we propagate correlation IDs across boundaries without modifying agent code? Can probabilistic data structures (Bloom filters, HyperLogLog) enable constant-memory correlation at millions of events per second?

\textbf{Tamper-Proof Telemetry Channels.} Securing the Data Plane → Cognitive Plane communication path against compromised production agents requires cryptographic guarantees. Open problems: How do we design append-only, authenticated telemetry streams using hardware security modules or trusted execution environments? Can blockchain-inspired techniques provide verifiable audit trails without performance overhead?

\textbf{Privacy-Preserving Capture.} TLS payload inspection reveals sensitive data (API keys, user prompts, proprietary algorithms), creating a fundamental tension between comprehensive visibility and data governance. Practical deployment faces compliance constraints (GDPR, SOC2) that prohibit full logging, while security requirements demand complete audit trails. Research challenges: How do we balance automated redaction (differential privacy, prompt sanitization) with maintaining sufficient context for anomaly detection? Can selective encryption preserve semantic understanding while protecting sensitive content? What policies determine which telemetry crosses organizational boundaries in federated observability deployments? Unlike existing tools that offer simple masking, production systems require adaptive privacy controls that adjust granularity based on risk context (e.g., full capture during incident investigation, redacted logging during normal operation).

\subsection{Cognitive Plane Challenges: Understanding at Scale}

\textbf{Semantic Reconstruction from System Events.} Bridging the abstraction gap between low-level telemetry (\texttt{execve("pylint")}, TLS payload bytes) and high-level intent ("validate code style before posting review") requires techniques from program synthesis, natural language inference, and causal reasoning. AgentSight~\cite{zheng2025agentsight} demonstrates boundary tracing via eBPF and TLS interception, successfully detecting prompt injections and reasoning loops. However, system-boundary observability faces three fundamental architectural limitations: \textbf{(1) Post-hoc visibility without intervention}: eBPF captures events \emph{after} execution, enabling forensics but not prevention—when an agent spawns a malicious subprocess, the damage occurs before observability can react. Real-time circuit-breaking requires application-layer hooks that system tracing cannot provide. \textbf{(2) Opaque model internals}: Kernel and network tracing cannot observe GPU memory (attention patterns, layer activations) or model serving internals (batching decisions, cache hits), missing critical signals for understanding \emph{why} models generated specific outputs. \textbf{(3) Multi-agent attribution ambiguity}: When containerized agents share host resources (file systems, network interfaces), syscall tracing cannot reliably attribute events to specific agents without application-provided correlation IDs. Key research questions remain: Can hybrid architectures combine system-boundary capture (tamper-resistant) with selective application instrumentation (semantic-rich)? How do we propagate correlation metadata across administrative boundaries (host kernel → container → model serving → agent runtime) without universal SDK adoption? What mechanisms enable real-time policy enforcement at system boundaries (e.g., eBPF-based syscall filtering triggered by semantic analysis)?

\textbf{Causal Inference for Agent Behavior.} Current systems correlate events but cannot answer counterfactuals: Why did the agent choose this tool over alternatives? What would have happened with different hyperparameters? Causal observability demands: identifying decision points in reasoning traces, instrumenting counterfactual branches in production agents for A/B testing, and constructing causal DAGs from observational telemetry.

\textbf{Adaptive Anomaly Detection.} As production agents evolve continuously, static anomaly baselines become obsolete. Research challenges: How do observability agents update their models of "normal behavior" without catastrophic forgetting? Can meta-learning enable few-shot adaptation to new agent types? How do we detect novel attack patterns (adversarial prompts, data poisoning) never seen during training?

\textbf{Non-Deterministic Behavior and Reproducibility.} Even with identical inputs, LLM agents produce varying outputs due to model randomness (temperature, sampling) and dynamic external data (RAG retrieval, API responses). This stochastic variability complicates debugging and trust: a failure observed once may not reproduce, while intermittent issues require statistical evidence rather than single-trace analysis. Research challenges: How do observability systems capture and represent distributional behaviors across multiple runs? Can probabilistic root-cause analysis identify failure modes that manifest in only 5\% of executions? What replay mechanisms enable reproducible debugging despite non-determinism (e.g., record-and-replay of random seeds, API responses, retrieved documents)?

\textbf{Session-Level Behavioral Analysis.} Production agents engage in multi-turn interactions where correctness emerges from session-wide coherence rather than per-step accuracy. Unlike single LLM call evaluation, session-level observability must assess whether agents maintain context retention, goal alignment, and instruction following across dozens of turns spanning minutes to hours. Research challenges: How do observability agents maintain session context across thousands of concurrent agents? Can hierarchical evaluation models (step-level + session-level + cross-session patterns) detect subtle drift in instruction following or persona consistency? What metrics quantify goal achievement in open-ended tasks where success criteria are implicit?

\subsection{Cross-Plane Challenges: Integration \& Standardization}

\textbf{Unified Semantic Conventions.} OpenTelemetry for agentic systems remains nascent~\cite{otelgenai,semconv}. While the GenAI project defines standard span names (e.g., \texttt{llm.call}, \texttt{tool.invocation}) and base attributes (\texttt{gen\_ai.request.model}, \texttt{gen\_ai.usage.tokens})~\cite{otelgenai}, community consensus is still needed on agent-specific reasoning semantics: span attributes capturing cognitive processes (\texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}), trace topology for multi-agent coordination (representing delegation, conflict, consensus), evaluation result schemas (hallucination scores, alignment metrics), and unified cost attribution across model inference, tool execution, and infrastructure layers. Standardization enables interoperability: conforming to semantic conventions allows switching between observability backends (LangSmith, Arize Phoenix, Langfuse) without rewriting instrumentation.

\textbf{Benchmarks and Evaluation.} The field lacks standardized evaluation frameworks. Critical needs: (1) datasets of real agentic failures with ground-truth root causes and counterfactual traces, (2) metrics quantifying semantic coverage (what fraction of reasoning intent is captured at each boundary?), (3) adversarial test suites evaluating tamper resistance against prompt injection and SDK evasion, (4) end-to-end benchmarks measuring detection latency, false positive rates, and remediation success across the full architecture.

\textbf{Securing Observability Infrastructure.} If the Cognitive Plane is compromised, attackers gain visibility into all production agents. Open problems: Byzantine-fault-tolerant consensus among observability agents, secure multi-party computation for collaborative diagnosis across organizations, and differential privacy guarantees for telemetry sharing that preserve detection accuracy while protecting sensitive data.

\section{Conclusion}

Production agentic systems demand a paradigm shift from application-layer instrumentation to unified agentic interfaces. We present a vision for observability built on two inseparable planes: a Data Plane capturing tamper-resistant telemetry at stable system boundaries (model inference, network, kernel, human feedback), and a Cognitive Plane deploying autonomous observability agents for semantic understanding at scale. Together, they address the fundamental challenge that \textbf{only autonomous intelligence can effectively observe, understand, and manage autonomous intelligence in production}. Realizing this vision requires sustained research across system-level capture, semantic reconstruction, and standardization: but the direction is clear: as agents transform software infrastructure, observability infrastructure must itself become agentic.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}

