
\documentclass[sigplan,screen,9pt]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{listings}     % For ASCII-art / code blocks
\usepackage{booktabs}     % Nicer tables
\usepackage{array}        % Column types
\usepackage{tabularx}     % Automatic column width
\usepackage{enumitem}     % Compact lists

\usepackage{newunicodechar} % Custom Unicode characters
\usepackage{fancyvrb}   % Enhanced verbatim environment

% Define box drawing characters (optional, ensures correct rendering)
\newunicodechar{┌}{\symbol{"250C}} % Top-left corner
\newunicodechar{┐}{\symbol{"2510}} % Top-right corner
\newunicodechar{└}{\symbol{"2514}} % Bottom-left corner
\newunicodechar{┘}{\symbol{"2518}} % Bottom-right corner
\newunicodechar{─}{\symbol{"2500}} % Horizontal line
\newunicodechar{│}{\symbol{"2502}} % Vertical line

\begin{document}

\title{Unified Agentic Interfaces is All You Need for AI Agent Observability}

\author{Yanpeng Hu}
\email{huyp@shanghaitech.edu.cn}
\affiliation{%
  \institution{ShanghaiTech University}
  \country{China}
}

\author{Yusheng Zheng}
\email{yzhen165@ucsc.edu}
\affiliation{%
  \institution{UC Santa Cruz}
  \country{USA}
}


\sloppy
\begin{abstract}
Production AI agent systems expose a fundamental observability mismatch: traditional monitoring assumes deterministic code with stable interfaces, while agents generate non-deterministic outputs through rapidly-evolving logic. Current solutions (APM tools, LLM-centric monitoring, and framework-specific SDKs) create fragmented silos that cannot capture semantic reasoning, resist tampering, or scale across multi-vendor ecosystems. We argue that \emph{agent observability fundamentally requires autonomous intelligence: LLM-powered observability agents are necessary to interpret semantic failures, adapt to evolving agent behaviors, and perform cross-layer causal reasoning at production scale--capabilities that humans and rule-based systems cannot provide}. This paper presents a vision for unified agentic interfaces through a two-plane architecture: a Data Plane capturing telemetry at stable system boundaries (syscalls, network, inference endpoints, human feedback), decoupling observability from agent internals; and a Cognitive Plane deploying autonomous observability agents for semantic understanding at production scale. We identify open research challenges across system-level capture, semantic reconstruction, and standardization required to realize this vision.
\end{abstract}


\maketitle



\section{The Agentic Observability Challenge}

\subsection{The Transformation: From Deterministic Code to Autonomous Reasoning}

AI-powered agentic systems are fundamentally changing how we build software infrastructure~\cite{wang2024survey,guo2024survey}. Frameworks like AutoGen~\cite{autogen}, LangChain~\cite{langchain}, Claude Code~\cite{claudecode}, and gemini-cli~\cite{geminicli} orchestrate large language models (LLMs) to autonomously execute complex workflows, including debugging production incidents, analyzing multi-modal data pipelines, coordinating distributed deployments, and making real-time operational decisions~\cite{tran2025survey}.

Consider a concrete example: an automated code review agent receives a pull request, analyzes the diff against project style guides, queries a vector database for similar past bugs, spawns subprocess tools to run linters and tests, coordinates with a security agent to check for vulnerabilities, and finally posts structured feedback. This workflow involves multiple LLM calls, external tool invocations, inter-agent communication, and persistent state, all orchestrated autonomously with minimal human intervention.

Yet despite rapid adoption in development environments, production deployment at scale faces three fundamental challenges that expose a critical gap in existing observability paradigms:

\textbf{Semantic Failures Replace Deterministic Errors.} The agent might hallucinate non-existent vulnerabilities, enter infinite loops, or forget guidelines mid-review. Unlike crashes or exceptions, these failures are \emph{semantic}: plausible but incorrect outputs requiring understanding of agent \emph{intent}. Building multi-agent systems without observability ``feels like debugging a black box''~\cite{petropavlov-medium,masood-medium}. Worse, prompt injection attacks~\cite{indirect-prompt-inject} let compromised agents evade their own logging. Traditional metrics (CPU, latency, 5xx errors) cannot capture these failure modes.

\textbf{Opaque Multi-Layer Costs.} The code review workflow incurs costs at every layer, including token usage for LLM calls, vector database queries, API calls to GitHub, and subprocess execution for linters. When agents spawn recursive sub-tasks or enter reasoning loops, costs can spiral unpredictably. Without unified visibility across this multi-layer stack, runaway expenses remain invisible until discovered in post-incident analysis.

\textbf{Fragmented Multi-Vendor Instrumentation.} Execution spans multiple domains (LLM APIs, orchestration frameworks, vector stores, subprocess calls, inter-agent communication), each with incompatible SDKs and logging formats, creating telemetry silos. Multi-agent coordination reports up to 15× higher token consumption than single-agent workflows~\cite{anthropic-multiagent}, while error propagation across handoffs creates cascading failures invisible to per-agent monitoring. Debugging requires correlating logs across five different systems with no unified trace.

\begin{table*}[t]
  \caption{Traditional vs. Agentic Observability: A Comparative Framework}
  \label{tab:diff}
  \centering
  \begin{tabular}{@{}p{3cm}p{5.5cm}p{5.5cm}@{}}
    \toprule
    \textbf{Aspect} &
    \textbf{Traditional Observability} &
    \textbf{Agentic Observability} \\
    \midrule
    Primary Goal &
    System health \& performance &
    Behavioral correctness, safety, \& trust \\
    Core Pillars &
    Metrics, Events, Logs, Traces (MELT)~\cite{li2022observability} &
    MELT + \textbf{Evaluations} + \textbf{Governance} \\
    Nature of Failures &
    Crashes, exceptions, latency spikes &
    ``Quiet failures'' (hallucinations, flawed logic, misuse of tools) \\
    System Behavior &
    Deterministic \& predictable &
    Non-deterministic \& emergent \\
    Key Question &
    ``Is the system working?'' &
    ``Is the system thinking correctly and acting appropriately?'' \\
    Core Unit of Analysis &
    Service/request trace &
    Agent decision path/trajectory graph \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Why Existing Observability Paradigms Cannot Scale}

These challenges reveal a fundamental mismatch between agent systems and existing observability approaches. Table~\ref{tab:diff} summarizes how agent observability differs qualitatively from traditional software monitoring. Three existing paradigms address parts of this problem, but none provides a complete solution:

\textbf{Traditional APM Is Operationally Blind to Semantics:} Classic application performance monitoring (APM) tools like Datadog and New Relic excel at detecting infrastructure failures such as crashes, 5xx errors, memory leaks, and latency spikes. But when our code review agent hallucinates a non-existent vulnerability, no error is thrown. CPU and memory remain normal. The only signal is \emph{semantic incorrectness}, which requires understanding natural language intent and reasoning quality, capabilities APM systems were never designed to provide.

\textbf{LLM-Centric Monitoring Stops at the Model Boundary:} Existing LLM monitoring solutions (prompt safety filters, hallucination detectors) focus on single-turn model interactions. They monitor token generation quality at the inference endpoint. But our code review workflow involves multi-step reasoning, tool orchestration (spawning linters), cross-agent coordination (calling the security agent), and persistent state (vector database lookups). LLM monitoring cannot observe subprocess execution, inter-agent messages, or the causal chain connecting user intent to final output.

\textbf{LLM Serving Observability Optimizes Infrastructure, Not Behavior:} LLM serving platforms monitor throughput, latency percentiles, GPU utilization, and SLO compliance, all infrastructure metrics for the inference layer. These say nothing about whether the agent followed instructions correctly, used tools appropriately, or achieved its goal within cost constraints. Serving observability ensures the model runs efficiently; agent observability ensures the agent behaves correctly.

\subsection{Two Fundamental Gaps}

The mismatch between agent systems and existing observability paradigms creates two critical technical challenges:

\textbf{The Instrumentation Gap: Agent Code Is Unstable:} Returning to our code review agent, suppose it initially uses \texttt{subprocess.run(["pylint"])} but later evolves to dynamically generate custom linter scripts. Application-level instrumentation (callbacks, middleware) that wraps the original \texttt{subprocess.run} call becomes obsolete. Worse, if the agent is compromised via prompt injection~\cite{indirect-prompt-inject}, it can modify its own logging code to hide malicious behavior. For example, it could write a bash script with exploit commands (not logged as harmful file I/O) and then execute it (appears as a normal tool call). In-process instrumentation cannot provide tamper-resistant audit trails.

\textbf{The Semantic Gap: System Events Lack Intent:} Conversely, observing only syscalls and network traffic shows \emph{what} happened (process spawned, bytes sent) but not \emph{why}. When our code review agent spawns \texttt{pylint}, syscall tracing records \texttt{execve("pylint", [...])}. But \emph{why} did the agent run it? What reasoning led to this decision? Traditional observability frameworks~\cite{sigelman2010dapper,majors2017observability} lack semantic primitives such as attributes like \texttt{agent.goal}, \texttt{reasoning.step\_id}, \texttt{tool.justification}, or anomaly detectors for semantic failures (contradictions, persona drift, instruction forgetting).

These gaps are complementary: application instrumentation provides semantics but is fragile and tamperable; system-boundary tracing is stable and tamper-resistant but semantically opaque. A complete solution must bridge both.

\section{Current Solutions: A Fragmented Landscape}

Having established the unique challenges of agent observability, we now survey existing solutions. Our analysis reveals a maturing ecosystem converging toward OpenTelemetry standards~\cite{otelgenai,Liu2025OTel}, yet fundamentally limited by reliance on application-layer instrumentation that cannot address the instrumentation and semantic gaps.

\subsection{Methodology: Ecosystem Survey}

We surveyed agentic observability tooling as of early 2025, examining both industrial deployments and academic research. Our analysis covers two areas: (1) \textbf{Industrial tools}, which are production-ready solutions providing SDKs, proxies, or specifications for agent framework integration (Table 2), and (2) \textbf{Academic research}, which includes foundational work on agent monitoring, interpretability, and evaluation that informs current tooling design.


\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{p{0.5cm} p{2.2cm} p{2.3cm} p{2.8cm} p{1.8cm} p{2.5cm}}
\toprule
\# & Tool / SDK (year) & Integration path & What it provides & License / model & Notes \\
\midrule
1 & \textbf{LangSmith}~\cite{langsmith} (2023) & Add \texttt{import langsmith} to LangChain/LangGraph apps & Request/response traces, prompt \& token stats, evaluations & SaaS, free tier & Tight LangChain integration; OTel export beta \\
2 & \textbf{Helicone}~\cite{helicone} (2023) & Reverse-proxy or Python/JS SDK & Logs OpenAI-style calls, cost/latency dashboards & OSS (MIT) + hosted & Proxy model requires no code changes \\
3 & \textbf{Traceloop}~\cite{traceloop} (2024) & One-line SDK import → OTel & OTel spans for prompts, tools, sub-calls & SaaS, free tier & Standard OTel data compatibility \\
4 & \textbf{Arize Phoenix}~\cite{phoenix} (2024) & \texttt{pip install}, OpenInference tracer & Local UI + vector store for traces, automatic evals & Apache-2.0 & Includes open-source UI for debugging \\
5 & \textbf{Langfuse}~\cite{langfuse} (2024) & SDK or OTel OTLP & Nested traces, cost metrics, prompt management & OSS (MIT) + cloud & Popular for RAG/multi-agent projects \\
6 & \textbf{WhyLabs LangKit}~\cite{whylabs} (2023) & Text metrics wrapper & Drift, toxicity, sentiment, PII detection & Apache-2.0 core & Focuses on text-quality metrics \\
7 & \textbf{PromptLayer}~\cite{promptlayer} (2022) & Decorator or proxy & Prompt chain timeline, diff \& replay & SaaS & Early solution, minimal code changes \\
8 & \textbf{Literal AI}~\cite{literalai} (2024) & Python SDK + UI & RAG-aware traces, eval experiments & OSS + SaaS & Targets chatbot product teams \\
9 & \textbf{W\&B Weave/Traces}~\cite{wandb} (2024) & \texttt{import weave} or SDK & Links to W\&B projects, captures code/IO & SaaS & Integrates with existing W\&B workflows \\
10 & \textbf{Honeycomb Gen-AI}~\cite{honeycomb} (2024) & Send OTel spans & Heat-maps on prompt spans, latency & SaaS & Built on mature trace store \\
11 & \textbf{OTel GenAI Conv.}~\cite{otelgenai} (2024) & Spec + Python lib & Standard span names for models/agents & Apache-2.0 & Provides semantic conventions \\
12 & \textbf{OpenInference}~\cite{openinference} (2023) & Tracer wrapper & JSON schema for traces & Apache-2.0 & Specification (not hosted service) \\
13 & \textbf{AgentOps}~\cite{agentops-tool} (2024) & Proxy injection into LLM calls & Time-travel debugging, multi-framework support (CrewAI, AutoGen) & OSS & Session replay across agent frameworks \\
14 & \textbf{TruLens}~\cite{trulens} (2024) & Wrapper (\texttt{TruLlama}) or SDK & Multi-turn session tracking, custom feedback functions & OSS + hosted & Evaluation-focused with feedback loops \\
15 & \textbf{Phospho}~\cite{phospho} (2024) & Log ingestion API & Clustering/labeling of LLM outputs, anomaly detection & OSS & Post-hoc NLP analytics on collected data \\
16 & \textbf{MLflow}~\cite{mlflow-genai} (2024) & \texttt{mlflow.autolog()} for LLMs & Experiment tracking, artifact logging (prompts/outputs) & Apache-2.0 & General MLOps extended to generative AI \\
17 & \textbf{Maxim AI}~\cite{maxim-ai} (2024) & One-line SDK integration & Agent trajectory visualization, cost/latency analytics & SaaS & Polished dashboard for production monitoring \\
18 & \textbf{Guardrails.AI}~\cite{guardrails-ai} (2023) & Input/output validators & Real-time safety checks (toxicity, PII), auto-retry on violations & OSS (Apache-2.0) & Observability through safety enforcement \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Key Findings: The Limits of Current Approaches}

\textbf{Industrial Tools:} Analysis of 18 production systems (Table 2) reveals critical limitations: all tools require application-level instrumentation, creating maintenance burden and tampering vulnerabilities; despite OpenTelemetry adoption by five tools~\cite{Liu2025OTel}, agent-specific attributes remain unstandardized; while tools like TruLens and Arize Phoenix provide LLM-powered evaluation, none explain why decisions were made through reasoning trace reconstruction; and no tool observes kernel syscalls, TLS payloads, or subprocess execution directly.

\textbf{Academic Research:} Four research threads address complementary aspects: agent monitoring frameworks~\cite{Rombaut2025Watson,Dong2024AgentOps} focus on trajectory logging but assume instrumented runtimes; mechanistic interpretability~\cite{Kim2025AgenticInterp} reveals model internal reasoning through attention analysis; evaluation methodologies~\cite{Moshkovich2025Pipeline} develop semantic correctness metrics but operate on logged outputs rather than system-boundary telemetry; and system-level observability~\cite{zheng2025agentsight} using eBPF to capture kernel events and TLS payloads, though this approach only observes post-hoc without real-time intervention, lacks standardized schemas for multi-agent coordination, and cannot observe ML inference stack internals.

These findings reflect a deeper problem: existing approaches inherit design assumptions from two incompatible paradigms, neither suited for production agentic systems.

\subsection{Redefining Agent Observability for Production Systems}

Current approaches define observability narrowly, missing critical production requirements. Academic definitions focus on individual agent internal consistency (beliefs, intentions, actions) but ignore system-level concerns like multi-layer costs and multi-agent coordination. Industrial tools provide model-centric input/output analysis, capturing inference-level telemetry but missing tool execution, inter-agent communication, and cross-layer causality. Production deployment demands system-level, multi-agent observability addressing cost transparency across all layers, tamper-resistant audit trails, multi-agent coordination visibility, and unified causal graphs linking intent to execution. This reveals why current solutions are inadequate: they optimize for single-agent, single-layer monitoring while production requires multi-agent, multi-layer, system-centric observability.

\textbf{The Path Forward:} Achieving production-grade agentic observability demands resolving two architectural tensions: (1) \emph{Where to capture telemetry?} Application instrumentation is semantically rich but fragile and tamperable; system boundaries are stable and tamper-resistant but semantically opaque. (2) \emph{Who analyzes telemetry?} Human operators understand intent but cannot scale to millions of events; rule-based systems scale but cannot interpret semantic failures. A complete solution must bridge both gaps simultaneously: capturing at stable interfaces while analyzing with autonomous intelligence. Therefore, we propose a two-plane architecture that unifies observability through stable system boundaries (Data Plane) while deploying autonomous intelligence for semantic understanding (Cognitive Plane). This architecture addresses the fundamental insight that only autonomous intelligence can effectively observe, understand, and manage autonomous intelligence in production.

\section{A Two-Plane Architecture for Agent Observability}

Production agent deployment exhibits three characteristics making traditional observability infeasible: heterogeneity (execution spans multiple administrative domains including LLM providers, agent frameworks, runtimes, and third-party tools, each with incompatible SDKs, making application-level coordination untenable), dynamism (agents modify their own logic continuously through prompt evolution, dynamic tool synthesis, and runtime feedback, making static instrumentation obsolete within days while enabling self-modification to bypass logging), and scale (thousands of agents generate millions of semantically-rich events per hour, creating a cognitive gap between raw telemetry and actionable insight that exceeds human capacity). These characteristics create two inescapable requirements:

\textbf{Requirement 1 (from heterogeneity and dynamism):} Observability must decouple from application internals, capturing telemetry at stable system boundaries that remain invariant across vendor changes, framework updates, and agent self-modification. This necessitates a Data Plane providing zero-instrumentation capture at kernel, network, and TLS interfaces, creating a unified foundation independent of agent implementation details.

\textbf{Requirement 2 (from scale and semantics):} Understanding agent behavior at production scale requires autonomous intelligence, including systems that interpret natural language prompts, correlate multi-layer telemetry, infer causal relationships, and adapt to evolving agent behaviors. Only LLM-powered systems can bridge the semantic gap between raw syscalls and agent intent. This necessitates a Cognitive Plane where specialized observability agents monitor, diagnose, and remediate other agents.

Critically, these planes are interdependent. The Data Plane provides tamper-resistant, unified telemetry that the Cognitive Plane requires for trustworthy analysis. The Cognitive Plane provides semantic understanding that makes Data Plane events actionable. Neither can function effectively alone. They form an integrated architecture where system-boundary capture enables intelligent understanding, and intelligent understanding validates system-boundary events. We now detail each plane's design.

\subsection{The Data Plane: Unified, Zero-Instrumentation Telemetry Capture}

Building on the AgentOps taxonomy~\cite{Dong2024AgentOps} that identifies key observability artifacts (goals, plans, tool outputs), we extend capture to stable OS/hardware boundaries (syscalls, TLS protocols, GPU APIs) that evolve slowly under vendor guarantees. Zero-instrumentation design eliminates SDK dependencies,\footnote{While frameworks like LlamaIndex offer "one-click" SDK integration (e.g., \texttt{set\_global\_handler("arize\_phoenix")}), these still require modifying agent codebases and remain vulnerable to tampering. The Data Plane's zero-instrumentation approach eliminates this dependency entirely.} operates at kernel/hardware level where compromised agents cannot falsify telemetry, and enables programmable interfaces for custom filters and correlation. The Data Plane organizes capture into four hierarchical layers:

\emph{Model Layer}: Captures GPU/CPU metrics, framework hooks, and model internals through mechanistic interpretability~\cite{Kim2025AgenticInterp} (attention patterns, layer activations) and dynamic GPU instrumentation~\cite{yang2025egpu}, bridging infrastructure monitoring and semantic transparency.

\emph{Network Layer}: Captures TLS-encrypted traffic between agents and external services using programmable interfaces like eBPF~\cite{zheng2025extending} to intercept prompts, reasoning traces, and API responses without proxies or SDK modifications.

\emph{System Layer}: Captures kernel syscalls revealing tool execution, file access, and resource consumption through eBPF tracing~\cite{brendangregg,ebpfio}, providing tamper-resistant visibility into agent actions.

\emph{Human Layer}: Captures human feedback, corrections, and interventions alongside agent responses, providing ground truth for evaluation and closing the observability loop.

While AgentSight~\cite{zheng2025agentsight} demonstrates eBPF-based boundary tracing, system-level observability alone cannot observe model internals or enable real-time intervention. The Data Plane addresses these limitations through architectural unification across four complementary layers, delivering framework neutrality, tamper resistance, cost transparency, and multi-vendor compatibility. However, raw telemetry alone cannot explain \emph{why} decisions were made or identify anomalous behavior, necessitating the Cognitive Plane.

\subsection{The Cognitive Plane: Why Only Agents Can Observe Agents}

Raw events (\texttt{execve("pylint")}, TLS payloads, token counts) cannot answer: \emph{Why} this decision? Is this anomalous? How should we respond? Building on Watson's cognitive observability~\cite{Rombaut2025Watson} and agentic interpretability~\cite{Kim2025AgenticInterp}, we argue only autonomous, LLM-powered systems provide this intelligence at production scale:

\textbf{Semantic failures require semantic understanding.} Agent failures are semantic incorrectness in natural language, not crashes. Detecting hallucinations or reasoning flaws requires context-dependent understanding that rule-based systems cannot provide. Watson~\cite{Rombaut2025Watson} demonstrates how observability agents retroactively infer reasoning traces, reconstructing why agents made specific decisions.

\textbf{Dynamic evolution demands continuous learning.} Agent behaviors evolve as prompts change and tools emerge. Static rulesets become obsolete within days. Observability agents learn from historical incidents, updating their models of normalcy as production agents evolve.

\textbf{Multi-layer causal reasoning exceeds human capacity.} A cost spike may trace to a reasoning loop caused by a database timeout from a network partition. Reconstructing such causal chains from millions of events demands hypothesis generation, cross-layer correlation, and counterfactual reasoning natural for LLM-powered diagnoser agents but impossible for humans at scale.

The two-plane design enforces strict separation across three dimensions: trust boundaries (Data Plane operates at privileged system layers inaccessible to production agents, preventing compromised agents from falsifying observability data), technology stacks (Data Plane requires low-level systems programming while Cognitive Plane requires LLM orchestration), and evolution rates (Data Plane interfaces evolve slowly with OS stability while Cognitive Plane adapts rapidly to changing agent behaviors). The planes form an inseparable architecture where system-boundary capture provides trusted telemetry while autonomous intelligence transforms it into actionable insights.

\section{Open Research Challenges}

The Data Plane faces challenges in correlating heterogeneous telemetry (GPU metrics, TLS payloads, syscalls, human feedback) across boundaries, ensuring tamper resistance through authenticated streams, and balancing privacy compliance with observability needs.

The Cognitive Plane must bridge low-level telemetry to high-level intent through causal inference, adaptive anomaly detection as agents evolve, probabilistic root-cause analysis for non-deterministic failures, and hierarchical evaluation across session boundaries.

Integration and standardization challenges include extending OpenTelemetry's GenAI conventions~\cite{otelgenai,semconv} with agent-specific semantics (goals, reasoning steps, multi-agent coordination), developing evaluation frameworks with ground-truth failure datasets, and securing cross-organization telemetry sharing.

\section{Conclusion}

Production agentic systems demand a paradigm shift from application-layer instrumentation to unified agentic interfaces. We present a vision for observability built on two inseparable planes: a Data Plane capturing tamper-resistant telemetry at stable system boundaries (model inference, network, kernel, human feedback), and a Cognitive Plane deploying autonomous observability agents for semantic understanding at scale. Together, they address the fundamental challenge that only autonomous intelligence can effectively observe, understand, and manage autonomous intelligence in production. Realizing this vision requires sustained research across system-level capture, semantic reconstruction, and standardization: but the direction is clear: as agents transform software infrastructure, observability infrastructure must itself become agentic.


\bibliographystyle{ACM-Reference-Format}
\bibliography{ai}

\end{document}

