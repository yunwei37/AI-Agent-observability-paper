\section{Motivation}

\subsection{The High Stakes of Agent Autonomy: Failure Scenarios}

The gap between agent capabilities and observability creates significant reliability and security risks. Consider these real-world failure scenarios:

\textbf{Unintended System Modification:} An AI agent tasked with code review misinterpreted its instructions and began modifying the production codebase it was supposed to analyze. Its actions—file modifications and \texttt{git} commits—were invoked through subprocesses, making them completely invisible to the application-level monitoring framework. The agent's reasoning process, captured in its prompts to the LLM, showed it believed it was "improving" the code, but the monitoring system could not correlate these intentions with the subsequent destructive actions.

\textbf{Costly Reasoning Loops:} A data analysis agent entered an infinite reasoning loop, repeatedly calling an expensive LLM API to solve an impossible constraint problem. The runaway process consumed thousands of dollars in API costs before manual intervention was possible, as framework-level logs gave no indication of the repetitive, non-productive cycle. Traditional monitoring showed only increasing API calls without understanding the semantic pattern of repeated failed attempts.

\textbf{Cross-Process Exploitation:} An agent compromised through prompt injection wrote a malicious shell script to \texttt{/tmp}, then executed it through standard tool APIs. The file creation appeared benign in logs, while the subsequent execution—which exfiltrated sensitive data—occurred in a subprocess completely invisible to the Python-based monitoring framework.

These incidents are symptomatic of a new class of threats unique to AI agents. The threat model includes \textbf{prompt injection attacks} where malicious instructions embedded in user input override safety guidelines, \textbf{goal drift} where optimization for metrics leads to unintended behaviors, \textbf{multi-agent coordination failures} where individual agents work at cross-purposes, and most concerning, \textbf{capability escalation} where agents write code to extend their own abilities, potentially circumventing safety measures.

\subsection{A Critical Analysis of Current AI Observability Solutions}

To address these challenges, several observability solutions have emerged, but each possesses fundamental limitations when applied to the failure scenarios above. We categorize them as follows:

\textbf{SDK-Based Instrumentation:} Solutions like LangSmith~\cite{langsmith} and Langfuse~\cite{langfuse} embed hooks directly into agent frameworks. This approach is brittle; it requires constant maintenance to keep pace with rapid framework evolution (e.g., LangChain's multiple breaking changes per month) and is blind to any operations that occur outside the instrumented code, such as executing a shell script. In our code review scenario, these tools would have captured the agent's LLM interactions but completely missed the subprocess-based file modifications. Crucially, SDK instrumentation assumes a cooperative agent and can be easily bypassed by compromised or malfunctioning agents.

\textbf{Proxy-Based Interception:} Tools like Helicone~\cite{helicone} and PromptLayer~\cite{promptlayer} monitor agent behavior by proxying LLM API calls. While this avoids code modification, it only captures network requests to LLMs, completely missing local activities like file operations, tool use, and subprocess execution. In our reasoning loop scenario, these tools would show repeated API calls but provide no insight into why the agent was stuck or what local operations it performed between calls. They cannot distinguish a benign request from one that leads to malicious local action.

\textbf{Generic System-Level Monitoring:} Security tools like Falco~\cite{falco} and Tracee~\cite{tracee} provide comprehensive system call visibility. However, they lack AI-specific context. In our cross-process exploitation scenario, they would report that a file was written to \texttt{/tmp} and later executed, but cannot link these events to the agent's preceding reasoning captured in a prompt. They fail to bridge the semantic gap between low-level system operations and high-level agent intentions, making it impossible to distinguish legitimate agent operations from malicious behavior.

No existing solution can simultaneously capture high-level semantic intent and low-level system actions, maintain visibility across process boundaries, and remain resilient to rapid framework changes. This critical gap—demonstrated by our failure scenarios—motivates our boundary tracing approach that observes agent behavior at stable system interfaces while maintaining the semantic context necessary to understand agent intentions.