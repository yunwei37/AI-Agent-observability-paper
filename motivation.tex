\section{Motivation}

\subsection{The High Stakes of Agent Autonomy: Failure Scenarios}

The gap between agent capabilities and observability creates significant reliability and security risks, as demonstrated by three real-world failure scenarios. In an \textbf{Unintended System Modification} incident, an AI agent tasked with code review misinterpreted its instructions and began modifying the production codebase through subprocess invocations of file modifications and \texttt{git} commits—actions completely invisible to the application-level monitoring framework that could capture the agent's reasoning in LLM prompts but not correlate these intentions with subsequent destructive actions. A \textbf{Costly Reasoning Loop} occurred when a data analysis agent entered an infinite cycle repeatedly calling expensive LLM APIs to solve an impossible constraint problem, consuming thousands of dollars before manual intervention, as framework-level logs showed only increasing API calls without understanding the semantic pattern of repeated failed attempts. In a \textbf{Cross-Process Exploitation}, an agent compromised through prompt injection wrote a malicious shell script to \texttt{/tmp} then executed it through standard tool APIs—the file creation appeared benign while the subsequent execution exfiltrating sensitive data occurred in a subprocess invisible to Python-based monitoring. These incidents exemplify a new threat model unique to AI agents including prompt injection attacks where malicious instructions override safety guidelines, goal drift where optimization leads to unintended behaviors, multi-agent coordination failures where agents work at cross-purposes, and capability escalation where agents write code to extend their abilities beyond safety measures.

\subsection{A Critical Analysis of Current AI Observability Solutions}

To address these challenges, several observability solutions have emerged, but each possesses fundamental limitations when applied to the failure scenarios above. \textbf{SDK-Based Instrumentation} solutions like LangSmith~\cite{langsmith} and Langfuse~\cite{langfuse} embed hooks directly into agent frameworks, requiring constant maintenance to keep pace with rapid framework evolution (e.g., LangChain's multiple breaking changes per month) and remaining blind to operations outside instrumented code—in our code review scenario, these tools would capture the agent's LLM interactions but completely miss subprocess-based file modifications, while assuming cooperative agents that can easily bypass instrumentation when compromised. \textbf{Proxy-Based Interception} tools like Helicone~\cite{helicone} and PromptLayer~\cite{promptlayer} monitor agent behavior by proxying LLM API calls, avoiding code modification but capturing only network requests to LLMs while missing local activities like file operations, tool use, and subprocess execution—in our reasoning loop scenario, these tools would show repeated API calls but provide no insight into why the agent was stuck or what local operations it performed between calls, unable to distinguish benign requests from those leading to malicious local action. \textbf{Generic System-Level Monitoring} security tools like Falco~\cite{falco} and Tracee~\cite{tracee} provide comprehensive system call visibility but lack AI-specific context—in our cross-process exploitation scenario, they would report that a file was written to \texttt{/tmp} and later executed, but cannot link these events to the agent's preceding reasoning captured in a prompt, failing to bridge the semantic gap between low-level system operations and high-level agent intentions. No existing solution can simultaneously capture high-level semantic intent and low-level system actions, maintain visibility across process boundaries, and remain resilient to rapid framework changes—this critical gap, demonstrated by our failure scenarios, motivates our boundary tracing approach that observes agent behavior at stable system interfaces while maintaining the semantic context necessary to understand agent intentions.