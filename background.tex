\section{Background}

\subsection{AI Agent Architecture}

AI agents represent a new class of software systems that combine language models with environmental interactions. These systems typically consist of three core components: (1) an LLM backend that provides reasoning capabilities, (2) a tool execution framework that enables system interactions, and (3) a control loop that orchestrates prompts, tool calls, and state management. Popular frameworks such as LangChain~\cite{langchain}, AutoGen~\cite{autogen}, and Claude Code implement variations of this architecture. The key characteristic distinguishing AI agents from traditional software is their ability to dynamically construct execution plans based on natural language objectives—an agent tasked with "analyze this dataset" might autonomously decide to install packages, write analysis scripts, execute them, and interpret results—all without predetermined logic paths. This flexibility comes from the LLM's ability to generate arbitrary code and command sequences.

\subsection{The Observability Challenge: A Departure from Traditional Software}

The autonomy of AI agents creates unprecedented observability challenges. Traditional Application Performance Monitoring (APM) assumes deterministic software with predefined execution paths that can be instrumented at development time. AI agents violate this assumption in three fundamental ways: \textbf{Dynamic Execution Paths} where an agent's sequence of operations emerges from LLM reasoning, not static code, making the same task solvable differently on each run and impossible to instrument all potential logic paths in advance; \textbf{Cross-Boundary Interactions} where agents frequently use tools to spawn subprocesses (\texttt{bash}, \texttt{python}), execute shell commands (\texttt{curl}, \texttt{git}), or interact with the filesystem, escaping the monitoring scope of their parent process and rendering traditional process-scoped instrumentation blind; and \textbf{The Semantic Gap} where a series of low-level system calls—like file reads and writes—is meaningless without context, requiring correlation with the agent's high-level intent locked within LLM interactions to understand whether operations constitute benign data analysis or malicious exfiltration. These fundamental differences demand a new approach to observability that can capture both the "why" (agent reasoning) and the "what" (system effects) of agent behavior.

\subsection{eBPF Technical Foundation}

eBPF (extended Berkeley Packet Filter) represents a fundamental advancement in kernel programmability, enabling safe execution of custom programs within the Linux kernel without modifying kernel source code or loading kernel modules~\cite{brendangregg}. Originally developed for packet filtering, eBPF has evolved into a general-purpose in-kernel virtual machine that powers modern observability, networking, and security tools~\cite{ebpfio,cilium}. For AI agent observability, eBPF provides unique capabilities that traditional monitoring approaches cannot match—it enables observation at the exact boundaries where agents interact with the system, capturing both high-level semantic information through TLS interception and low-level system behavior through syscall monitoring with minimal performance impact. The eBPF safety model is crucial for production deployment: the kernel verifier performs exhaustive analysis of eBPF programs before loading, ensuring memory safety through bounds-checked pointer arithmetic, proving program termination by prohibiting unbounded loops, enforcing resource limits on stack usage and execution time, and enabling type safety through BTF (BPF Type Format) for kernel version compatibility~\cite{kerneldoc}. These guarantees allow eBPF programs to run safely in production environments handling critical workloads.

\section{Motivation}

\subsection{The High Stakes of Agent Autonomy: Failure Scenarios}

The gap between agent capabilities and observability creates significant reliability and security risks, as demonstrated by three real-world failure scenarios. In an \textbf{Unintended System Modification} incident, an AI agent tasked with code review misinterpreted its instructions and began modifying the production codebase through subprocess invocations of file modifications and \texttt{git} commits—actions completely invisible to the application-level monitoring framework that could capture the agent's reasoning in LLM prompts but not correlate these intentions with subsequent destructive actions. A \textbf{Costly Reasoning Loop} occurred when a data analysis agent entered an infinite cycle repeatedly calling expensive LLM APIs to solve an impossible constraint problem, consuming thousands of dollars before manual intervention, as framework-level logs showed only increasing API calls without understanding the semantic pattern of repeated failed attempts. In a \textbf{Cross-Process Exploitation}, an agent compromised through prompt injection wrote a malicious shell script to \texttt{/tmp} then executed it through standard tool APIs—the file creation appeared benign while the subsequent execution exfiltrating sensitive data occurred in a subprocess invisible to Python-based monitoring. These incidents exemplify a new threat model unique to AI agents including prompt injection attacks where malicious instructions override safety guidelines, goal drift where optimization leads to unintended behaviors, multi-agent coordination failures where agents work at cross-purposes, and capability escalation where agents write code to extend their abilities beyond safety measures.

\subsection{A Critical Analysis of Current AI Observability Solutions}

To address these challenges, several observability solutions have emerged, but each possesses fundamental limitations when applied to the failure scenarios above. \textbf{SDK-Based Instrumentation} solutions like LangSmith~\cite{langsmith} and Langfuse~\cite{langfuse} embed hooks directly into agent frameworks, requiring constant maintenance to keep pace with rapid framework evolution (e.g., LangChain's multiple breaking changes per month) and remaining blind to operations outside instrumented code—in our code review scenario, these tools would capture the agent's LLM interactions but completely miss subprocess-based file modifications, while assuming cooperative agents that can easily bypass instrumentation when compromised. \textbf{Proxy-Based Interception} tools like Helicone~\cite{helicone} and PromptLayer~\cite{promptlayer} monitor agent behavior by proxying LLM API calls, avoiding code modification but capturing only network requests to LLMs while missing local activities like file operations, tool use, and subprocess execution—in our reasoning loop scenario, these tools would show repeated API calls but provide no insight into why the agent was stuck or what local operations it performed between calls, unable to distinguish benign requests from those leading to malicious local action. \textbf{Generic System-Level Monitoring} security tools like Falco~\cite{falco} and Tracee~\cite{tracee} provide comprehensive system call visibility but lack AI-specific context—in our cross-process exploitation scenario, they would report that a file was written to \texttt{/tmp} and later executed, but cannot link these events to the agent's preceding reasoning captured in a prompt, failing to bridge the semantic gap between low-level system operations and high-level agent intentions. No existing solution can simultaneously capture high-level semantic intent and low-level system actions, maintain visibility across process boundaries, and remain resilient to rapid framework changes—this critical gap, demonstrated by our failure scenarios, motivates our boundary tracing approach that observes agent behavior at stable system interfaces while maintaining the semantic context necessary to understand agent intentions.
